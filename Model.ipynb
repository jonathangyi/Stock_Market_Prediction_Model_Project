{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for AAPL...\n",
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetched successfully. Shape: (451, 10)\n",
      "Preparing data...\n",
      "Training data shape: X=(312, 60, 6), y=(312,)\n",
      "Testing data shape: X=(79, 60, 6), y=(79,)\n",
      "Building model...\n",
      "Model built successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">42,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,275</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m42,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m30,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │         \u001b[38;5;34m1,275\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m26\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">74,301</span> (290.24 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m74,301\u001b[0m (290.24 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">74,301</span> (290.24 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m74,301\u001b[0m (290.24 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Training model...\n",
      "Epoch 1/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1334 - mae: 0.2728"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 187ms/step - loss: 0.1271 - mae: 0.2651 - val_loss: 0.0160 - val_mae: 0.1078\n",
      "Epoch 2/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0218 - mae: 0.1173"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0217 - mae: 0.1171 - val_loss: 0.0119 - val_mae: 0.0890\n",
      "Epoch 3/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0132 - mae: 0.0986"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - loss: 0.0132 - mae: 0.0984 - val_loss: 0.0062 - val_mae: 0.0627\n",
      "Epoch 4/100\n",
      "\u001b[1m8/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0123 - mae: 0.0867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - loss: 0.0122 - mae: 0.0860 - val_loss: 0.0057 - val_mae: 0.0612\n",
      "Epoch 5/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - loss: 0.0093 - mae: 0.0774 - val_loss: 0.0066 - val_mae: 0.0682\n",
      "Epoch 6/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0073 - mae: 0.0681 - val_loss: 0.0070 - val_mae: 0.0705\n",
      "Epoch 7/100\n",
      "\u001b[1m8/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0074 - mae: 0.0710"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - loss: 0.0074 - mae: 0.0707 - val_loss: 0.0053 - val_mae: 0.0626\n",
      "Epoch 8/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0071 - mae: 0.0662 - val_loss: 0.0104 - val_mae: 0.0864\n",
      "Epoch 9/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0061 - mae: 0.0620 - val_loss: 0.0060 - val_mae: 0.0657\n",
      "Epoch 10/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - loss: 0.0057 - mae: 0.0597 - val_loss: 0.0055 - val_mae: 0.0637\n",
      "Epoch 11/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0076 - mae: 0.0690"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - loss: 0.0076 - mae: 0.0688 - val_loss: 0.0051 - val_mae: 0.0608\n",
      "Epoch 12/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - loss: 0.0066 - mae: 0.0639 - val_loss: 0.0065 - val_mae: 0.0683\n",
      "Epoch 13/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0063 - mae: 0.0626 - val_loss: 0.0054 - val_mae: 0.0627\n",
      "Epoch 14/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - loss: 0.0058 - mae: 0.0604 - val_loss: 0.0100 - val_mae: 0.0868\n",
      "Epoch 15/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0059 - mae: 0.0608 - val_loss: 0.0112 - val_mae: 0.0916\n",
      "Epoch 16/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - loss: 0.0056 - mae: 0.0580 - val_loss: 0.0059 - val_mae: 0.0598\n",
      "Epoch 17/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - loss: 0.0067 - mae: 0.0643 - val_loss: 0.0073 - val_mae: 0.0747\n",
      "Epoch 18/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - loss: 0.0067 - mae: 0.0648 - val_loss: 0.0163 - val_mae: 0.1119\n",
      "Epoch 19/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0069 - mae: 0.0660 - val_loss: 0.0054 - val_mae: 0.0631\n",
      "Epoch 20/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0046 - mae: 0.0542 - val_loss: 0.0053 - val_mae: 0.0628\n",
      "Epoch 21/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - loss: 0.0057 - mae: 0.0580 - val_loss: 0.0123 - val_mae: 0.0966\n",
      "Epoch 22/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0056 - mae: 0.0601 - val_loss: 0.0075 - val_mae: 0.0757\n",
      "Epoch 23/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0048 - mae: 0.0559"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - loss: 0.0048 - mae: 0.0558 - val_loss: 0.0047 - val_mae: 0.0577\n",
      "Epoch 24/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0052 - mae: 0.0580 - val_loss: 0.0065 - val_mae: 0.0702\n",
      "Epoch 25/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - loss: 0.0042 - mae: 0.0498 - val_loss: 0.0057 - val_mae: 0.0655\n",
      "Epoch 26/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0043 - mae: 0.0515 - val_loss: 0.0069 - val_mae: 0.0726\n",
      "Epoch 27/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - loss: 0.0040 - mae: 0.0491 - val_loss: 0.0065 - val_mae: 0.0709\n",
      "Epoch 28/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0047 - mae: 0.0537 - val_loss: 0.0073 - val_mae: 0.0753\n",
      "Epoch 29/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0043 - mae: 0.0489 - val_loss: 0.0137 - val_mae: 0.1026\n",
      "Epoch 30/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0047 - mae: 0.0546 - val_loss: 0.0205 - val_mae: 0.1285\n",
      "Epoch 31/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0043 - mae: 0.0493 - val_loss: 0.0186 - val_mae: 0.1211\n",
      "Epoch 32/100\n",
      "\u001b[1m8/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0052 - mae: 0.0563"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - loss: 0.0051 - mae: 0.0558 - val_loss: 0.0046 - val_mae: 0.0582\n",
      "Epoch 33/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0036 - mae: 0.0477 - val_loss: 0.0066 - val_mae: 0.0704\n",
      "Epoch 34/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - loss: 0.0038 - mae: 0.0472 - val_loss: 0.0133 - val_mae: 0.1006\n",
      "Epoch 35/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - loss: 0.0044 - mae: 0.0518 - val_loss: 0.0135 - val_mae: 0.1020\n",
      "Epoch 36/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - loss: 0.0048 - mae: 0.0513 - val_loss: 0.0132 - val_mae: 0.1001\n",
      "Epoch 37/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - loss: 0.0044 - mae: 0.0502 - val_loss: 0.0074 - val_mae: 0.0750\n",
      "Epoch 38/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - loss: 0.0043 - mae: 0.0513 - val_loss: 0.0085 - val_mae: 0.0799\n",
      "Epoch 39/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0045 - mae: 0.0531 - val_loss: 0.0122 - val_mae: 0.0962\n",
      "Epoch 40/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0041 - mae: 0.0486 - val_loss: 0.0118 - val_mae: 0.0950\n",
      "Epoch 41/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - loss: 0.0036 - mae: 0.0461 - val_loss: 0.0268 - val_mae: 0.1515\n",
      "Epoch 42/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - loss: 0.0045 - mae: 0.0517 - val_loss: 0.0079 - val_mae: 0.0781\n",
      "Epoch 43/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 0.0034 - mae: 0.0455 - val_loss: 0.0053 - val_mae: 0.0626\n",
      "Epoch 44/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0031 - mae: 0.0427 - val_loss: 0.0079 - val_mae: 0.0766\n",
      "Epoch 45/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - loss: 0.0038 - mae: 0.0471 - val_loss: 0.0088 - val_mae: 0.0811\n",
      "Epoch 46/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0036 - mae: 0.0457 - val_loss: 0.0141 - val_mae: 0.1036\n",
      "Epoch 47/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0038 - mae: 0.0480 - val_loss: 0.0048 - val_mae: 0.0595\n",
      "Model training completed!\n",
      "Evaluating model...\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Model Evaluation Metrics:\n",
      "Training RMSE: 5.5141\n",
      "Testing RMSE: 10.8833\n",
      "Training MAE: 4.3353\n",
      "Testing MAE: 7.7924\n",
      "Training R²: 0.9512\n",
      "Testing R²: 0.5809\n",
      "Predicting next 10 days...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Future Predictions (Next 10 days):\n",
      "Day 1: $197.30\n",
      "Day 2: $197.55\n",
      "Day 3: $197.79\n",
      "Day 4: $198.01\n",
      "Day 5: $198.19\n",
      "Day 6: $198.33\n",
      "Day 7: $198.45\n",
      "Day 8: $198.54\n",
      "Day 9: $198.61\n",
      "Day 10: $198.67\n",
      "Model saved to AAPL_stock_model.h5\n",
      "Scaler saved to AAPL_scaler.pkl\n",
      "Parameters saved to AAPL_params.pkl\n",
      "\n",
      "Training completed for AAPL\n",
      "Model files saved and ready for use in another notebook!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class StockPredictor:\n",
    "    def __init__(self, ticker_symbol, sequence_length=60, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Initialize the Stock Predictor\n",
    "        \n",
    "        Args:\n",
    "            ticker_symbol (str): Stock ticker symbol (e.g., 'AAPL', 'GOOGL')\n",
    "            sequence_length (int): Number of days to look back for prediction\n",
    "            test_size (float): Proportion of data to use for testing\n",
    "        \"\"\"\n",
    "        self.ticker = ticker_symbol\n",
    "        self.sequence_length = sequence_length\n",
    "        self.test_size = test_size\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.model = None\n",
    "        self.data = None\n",
    "        self.scaled_data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        \n",
    "    def fetch_data(self, period=\"2y\"):\n",
    "        \"\"\"\n",
    "        Fetch stock data from Yahoo Finance\n",
    "        \n",
    "        Args:\n",
    "            period (str): Time period for data ('1y', '2y', '5y', 'max')\n",
    "        \"\"\"\n",
    "        print(f\"Fetching data for {self.ticker}...\")\n",
    "        self.data = yf.download(self.ticker, period=period)\n",
    "        \n",
    "        if self.data.empty:\n",
    "            raise ValueError(f\"No data found for ticker {self.ticker}\")\n",
    "            \n",
    "        # Add technical indicators\n",
    "        self.data['MA_5'] = self.data['Close'].rolling(window=5).mean()\n",
    "        self.data['MA_20'] = self.data['Close'].rolling(window=20).mean()\n",
    "        self.data['MA_50'] = self.data['Close'].rolling(window=50).mean()\n",
    "        self.data['RSI'] = self._calculate_rsi(self.data['Close'])\n",
    "        self.data['Volume_MA'] = self.data['Volume'].rolling(window=20).mean()\n",
    "        \n",
    "        # Remove NaN values\n",
    "        self.data = self.data.dropna()\n",
    "        print(f\"Data fetched successfully. Shape: {self.data.shape}\")\n",
    "        \n",
    "    def _calculate_rsi(self, prices, window=14):\n",
    "        \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    def prepare_data(self, features=['Close', 'Volume', 'MA_5', 'MA_20', 'MA_50', 'RSI']):\n",
    "        \"\"\"\n",
    "        Prepare data for training\n",
    "        \n",
    "        Args:\n",
    "            features (list): List of features to use for prediction\n",
    "        \"\"\"\n",
    "        print(\"Preparing data...\")\n",
    "        \n",
    "        # Select features\n",
    "        feature_data = self.data[features].values\n",
    "        \n",
    "        # Scale the data\n",
    "        self.scaled_data = self.scaler.fit_transform(feature_data)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(self.sequence_length, len(self.scaled_data)):\n",
    "            X.append(self.scaled_data[i-self.sequence_length:i])\n",
    "            y.append(self.scaled_data[i, 0])  # Predict 'Close' price (first feature)\n",
    "            \n",
    "        X, y = np.array(X), np.array(y)\n",
    "        \n",
    "        # Split data\n",
    "        split_idx = int(len(X) * (1 - self.test_size))\n",
    "        self.X_train = X[:split_idx]\n",
    "        self.X_test = X[split_idx:]\n",
    "        self.y_train = y[:split_idx]\n",
    "        self.y_test = y[split_idx:]\n",
    "        \n",
    "        print(f\"Training data shape: X={self.X_train.shape}, y={self.y_train.shape}\")\n",
    "        print(f\"Testing data shape: X={self.X_test.shape}, y={self.y_test.shape}\")\n",
    "        \n",
    "    def build_model(self, lstm_units=[50, 50], dropout_rate=0.2):\n",
    "        \"\"\"\n",
    "        Build LSTM model\n",
    "        \n",
    "        Args:\n",
    "            lstm_units (list): Number of units in each LSTM layer\n",
    "            dropout_rate (float): Dropout rate for regularization\n",
    "        \"\"\"\n",
    "        print(\"Building model...\")\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        \n",
    "        # First LSTM layer\n",
    "        self.model.add(LSTM(units=lstm_units[0], \n",
    "                           return_sequences=True, \n",
    "                           input_shape=(self.X_train.shape[1], self.X_train.shape[2])))\n",
    "        self.model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        # Additional LSTM layers\n",
    "        for i in range(1, len(lstm_units)):\n",
    "            return_seq = i < len(lstm_units) - 1\n",
    "            self.model.add(LSTM(units=lstm_units[i], return_sequences=return_seq))\n",
    "            self.model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        # Dense layer\n",
    "        self.model.add(Dense(units=25))\n",
    "        self.model.add(Dense(units=1))\n",
    "        \n",
    "        # Compile model\n",
    "        self.model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                          loss='mean_squared_error',\n",
    "                          metrics=['mae'])\n",
    "        \n",
    "        print(\"Model built successfully!\")\n",
    "        print(self.model.summary())\n",
    "        \n",
    "    def train_model(self, epochs=100, batch_size=32, validation_split=0.1):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \n",
    "        Args:\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size for training\n",
    "            validation_split (float): Proportion of training data to use for validation\n",
    "        \"\"\"\n",
    "        print(\"Training model...\")\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        model_checkpoint = ModelCheckpoint(f'{self.ticker}_best_model.h5', \n",
    "                                         monitor='val_loss', \n",
    "                                         save_best_only=True)\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[early_stopping, model_checkpoint],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"Model training completed!\")\n",
    "        return history\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        train_predictions = self.model.predict(self.X_train)\n",
    "        test_predictions = self.model.predict(self.X_test)\n",
    "        \n",
    "        # Inverse transform predictions\n",
    "        train_pred_original = self.scaler.inverse_transform(\n",
    "            np.concatenate([train_predictions, np.zeros((len(train_predictions), self.scaled_data.shape[1]-1))], axis=1)\n",
    "        )[:, 0]\n",
    "        \n",
    "        test_pred_original = self.scaler.inverse_transform(\n",
    "            np.concatenate([test_predictions, np.zeros((len(test_predictions), self.scaled_data.shape[1]-1))], axis=1)\n",
    "        )[:, 0]\n",
    "        \n",
    "        # Inverse transform actual values\n",
    "        train_actual_original = self.scaler.inverse_transform(\n",
    "            np.concatenate([self.y_train.reshape(-1, 1), np.zeros((len(self.y_train), self.scaled_data.shape[1]-1))], axis=1)\n",
    "        )[:, 0]\n",
    "        \n",
    "        test_actual_original = self.scaler.inverse_transform(\n",
    "            np.concatenate([self.y_test.reshape(-1, 1), np.zeros((len(self.y_test), self.scaled_data.shape[1]-1))], axis=1)\n",
    "        )[:, 0]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_rmse = np.sqrt(mean_squared_error(train_actual_original, train_pred_original))\n",
    "        test_rmse = np.sqrt(mean_squared_error(test_actual_original, test_pred_original))\n",
    "        train_mae = mean_absolute_error(train_actual_original, train_pred_original)\n",
    "        test_mae = mean_absolute_error(test_actual_original, test_pred_original)\n",
    "        train_r2 = r2_score(train_actual_original, train_pred_original)\n",
    "        test_r2 = r2_score(test_actual_original, test_pred_original)\n",
    "        \n",
    "        metrics = {\n",
    "            'train_rmse': train_rmse,\n",
    "            'test_rmse': test_rmse,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2\n",
    "        }\n",
    "        \n",
    "        print(\"Model Evaluation Metrics:\")\n",
    "        print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "        print(f\"Testing RMSE: {test_rmse:.4f}\")\n",
    "        print(f\"Training MAE: {train_mae:.4f}\")\n",
    "        print(f\"Testing MAE: {test_mae:.4f}\")\n",
    "        print(f\"Training R²: {train_r2:.4f}\")\n",
    "        print(f\"Testing R²: {test_r2:.4f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def predict_future(self, days=10):\n",
    "        \"\"\"\n",
    "        Predict future stock prices\n",
    "        \n",
    "        Args:\n",
    "            days (int): Number of days to predict\n",
    "        \"\"\"\n",
    "        print(f\"Predicting next {days} days...\")\n",
    "        \n",
    "        # Get last sequence from scaled data\n",
    "        last_sequence = self.scaled_data[-self.sequence_length:]\n",
    "        predictions = []\n",
    "        \n",
    "        current_sequence = last_sequence.copy()\n",
    "        \n",
    "        for _ in range(days):\n",
    "            # Reshape for prediction\n",
    "            pred_input = current_sequence.reshape(1, self.sequence_length, self.scaled_data.shape[1])\n",
    "            \n",
    "            # Make prediction\n",
    "            pred = self.model.predict(pred_input, verbose=0)\n",
    "            predictions.append(pred[0, 0])\n",
    "            \n",
    "            # Update sequence (assume other features remain constant)\n",
    "            new_row = current_sequence[-1].copy()\n",
    "            new_row[0] = pred[0, 0]  # Update close price\n",
    "            \n",
    "            # Shift sequence\n",
    "            current_sequence = np.vstack([current_sequence[1:], new_row])\n",
    "        \n",
    "        # Inverse transform predictions\n",
    "        predictions = np.array(predictions).reshape(-1, 1)\n",
    "        dummy_features = np.zeros((len(predictions), self.scaled_data.shape[1] - 1))\n",
    "        predictions_full = np.concatenate([predictions, dummy_features], axis=1)\n",
    "        predictions_original = self.scaler.inverse_transform(predictions_full)[:, 0]\n",
    "        \n",
    "        return predictions_original\n",
    "    \n",
    "    def save_model(self, model_path=None, scaler_path=None):\n",
    "        \"\"\"Save model and scaler\"\"\"\n",
    "        if model_path is None:\n",
    "            model_path = f'{self.ticker}_stock_model.h5'\n",
    "        if scaler_path is None:\n",
    "            scaler_path = f'{self.ticker}_scaler.pkl'\n",
    "            \n",
    "        self.model.save(model_path)\n",
    "        joblib.dump(self.scaler, scaler_path)\n",
    "        \n",
    "        # Save model parameters\n",
    "        params = {\n",
    "            'ticker': self.ticker,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'test_size': self.test_size,\n",
    "            'data_shape': self.scaled_data.shape if self.scaled_data is not None else None\n",
    "        }\n",
    "        joblib.dump(params, f'{self.ticker}_params.pkl')\n",
    "        \n",
    "        print(f\"Model saved to {model_path}\")\n",
    "        print(f\"Scaler saved to {scaler_path}\")\n",
    "        print(f\"Parameters saved to {self.ticker}_params.pkl\")\n",
    "    \n",
    "    def load_model(self, model_path=None, scaler_path=None):\n",
    "        \"\"\"Load model and scaler\"\"\"\n",
    "        if model_path is None:\n",
    "            model_path = f'{self.ticker}_stock_model.h5'\n",
    "        if scaler_path is None:\n",
    "            scaler_path = f'{self.ticker}_scaler.pkl'\n",
    "            \n",
    "        self.model = load_model(model_path)\n",
    "        self.scaler = joblib.load(scaler_path)\n",
    "        \n",
    "        # Load parameters\n",
    "        params = joblib.load(f'{self.ticker}_params.pkl')\n",
    "        self.ticker = params['ticker']\n",
    "        self.sequence_length = params['sequence_length']\n",
    "        self.test_size = params['test_size']\n",
    "        \n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "        print(f\"Scaler loaded from {scaler_path}\")\n",
    "\n",
    "# Example usage and training script\n",
    "def train_stock_model(ticker_symbol, save_model=True):\n",
    "    \"\"\"\n",
    "    Complete training pipeline\n",
    "    \n",
    "    Args:\n",
    "        ticker_symbol (str): Stock ticker symbol\n",
    "        save_model (bool): Whether to save the trained model\n",
    "    \"\"\"\n",
    "    # Initialize predictor\n",
    "    predictor = StockPredictor(ticker_symbol)\n",
    "    \n",
    "    # Fetch and prepare data\n",
    "    predictor.fetch_data(period=\"2y\")\n",
    "    predictor.prepare_data()\n",
    "    \n",
    "    # Build and train model\n",
    "    predictor.build_model(lstm_units=[100, 50], dropout_rate=0.2)\n",
    "    history = predictor.train_model(epochs=100, batch_size=32)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = predictor.evaluate_model()\n",
    "    \n",
    "    # Make future predictions\n",
    "    future_predictions = predictor.predict_future(days=10)\n",
    "    \n",
    "    print(\"\\nFuture Predictions (Next 10 days):\")\n",
    "    for i, pred in enumerate(future_predictions, 1):\n",
    "        print(f\"Day {i}: ${pred:.2f}\")\n",
    "    \n",
    "    # Save model if requested\n",
    "    if save_model:\n",
    "        predictor.save_model()\n",
    "    \n",
    "    return predictor, history, metrics, future_predictions\n",
    "\n",
    "# Function to load and use saved model\n",
    "def load_and_predict(ticker_symbol, days=10):\n",
    "    \"\"\"\n",
    "    Load saved model and make predictions\n",
    "    \n",
    "    Args:\n",
    "        ticker_symbol (str): Stock ticker symbol\n",
    "        days (int): Number of days to predict\n",
    "    \"\"\"\n",
    "    predictor = StockPredictor(ticker_symbol)\n",
    "    predictor.load_model()\n",
    "    \n",
    "    # Fetch recent data for prediction\n",
    "    predictor.fetch_data(period=\"1y\")\n",
    "    predictor.prepare_data()\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = predictor.predict_future(days=days)\n",
    "    \n",
    "    print(f\"\\nPredictions for {ticker_symbol} (Next {days} days):\")\n",
    "    for i, pred in enumerate(predictions, 1):\n",
    "        print(f\"Day {i}: ${pred:.2f}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Train model for Apple stock\n",
    "    ticker = \"AAPL\"\n",
    "    predictor, history, metrics, predictions = train_stock_model(ticker, save_model=True)\n",
    "    \n",
    "    print(f\"\\nTraining completed for {ticker}\")\n",
    "    print(\"Model files saved and ready for use in another notebook!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

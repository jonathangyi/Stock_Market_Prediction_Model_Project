{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Enhanced Stock Predictor...\n",
      "This will take longer but should achieve better MAE!\n",
      "Attempting to fetch data for INTC using yfinance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['INTC']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yfinance failed: yfinance returned empty data.\n",
      "Falling back to Alpha Vantage for INTC...\n",
      "Data fetched from Alpha Vantage.\n",
      "Final data shape: (6243, 28)\n",
      "Preparing enhanced data...\n",
      "Using 27 features: ['Close', 'Open', 'High', 'Low', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'MA_50', 'MA_200', 'RSI', 'MACD', 'MACD_signal', 'BB_upper', 'BB_lower', 'ATR', 'Price_Change', 'High_Low_Ratio', 'Volume_Ratio', 'Close_lag_1', 'Close_lag_2', 'Close_lag_3', 'Close_lag_5', 'Volume_lag_1', 'Volume_lag_2', 'Volume_lag_3', 'Volume_lag_5']\n",
      "Training data shape: X=(5204, 120, 27), y=(5204,)\n",
      "Testing data shape: X=(919, 120, 27), y=(919,)\n",
      "Building enhanced model: lstm_advanced\n",
      "Enhanced model built successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">159,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_20          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,608</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_22          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_23          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_24          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_8 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m159,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_20          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_9 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m164,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_14 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m20,608\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_22          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_23          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_24          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">351,489</span> (1.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m351,489\u001b[0m (1.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">350,465</span> (1.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m350,465\u001b[0m (1.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> (4.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,024\u001b[0m (4.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Training enhanced model...\n",
      "Epoch 1/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 94ms/step - loss: 5.7241 - mae: 0.6020 - mse: 0.6237 - val_loss: 4.1954 - val_mae: 0.9456 - val_mse: 1.0491 - learning_rate: 5.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 3.2595 - mae: 0.3276 - mse: 0.1757 - val_loss: 1.9890 - val_mae: 0.5620 - val_mse: 0.4764 - learning_rate: 5.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 1.4574 - mae: 0.2546 - mse: 0.1099 - val_loss: 1.0329 - val_mae: 0.9068 - val_mse: 0.9768 - learning_rate: 5.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.4698 - mae: 0.2276 - mse: 0.0885 - val_loss: 0.8648 - val_mae: 1.2023 - val_mse: 1.5909 - learning_rate: 5.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.1463 - mae: 0.2012 - mse: 0.0716 - val_loss: 0.3459 - val_mae: 0.6909 - val_mse: 0.6394 - learning_rate: 5.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0631 - mae: 0.1771 - mse: 0.0567 - val_loss: 0.1159 - val_mae: 0.3517 - val_mse: 0.1780 - learning_rate: 5.0000e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0533 - mae: 0.1658 - mse: 0.0540 - val_loss: 0.1517 - val_mae: 0.4230 - val_mse: 0.2553 - learning_rate: 5.0000e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0477 - mae: 0.1603 - mse: 0.0497 - val_loss: 0.1255 - val_mae: 0.3680 - val_mse: 0.2130 - learning_rate: 5.0000e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - loss: 0.0381 - mae: 0.1496 - mse: 0.0403 - val_loss: 0.1617 - val_mae: 0.4197 - val_mse: 0.2864 - learning_rate: 5.0000e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0398 - mae: 0.1530 - mse: 0.0424 - val_loss: 0.2403 - val_mae: 0.5525 - val_mse: 0.4556 - learning_rate: 5.0000e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0391 - mae: 0.1536 - mse: 0.0432 - val_loss: 0.1711 - val_mae: 0.4421 - val_mse: 0.3138 - learning_rate: 5.0000e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - loss: 0.0325 - mae: 0.1354 - mse: 0.0335 - val_loss: 0.2001 - val_mae: 0.4852 - val_mse: 0.3704 - learning_rate: 5.0000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - loss: 0.0374 - mae: 0.1447 - mse: 0.0377 - val_loss: 0.1966 - val_mae: 0.5112 - val_mse: 0.3534 - learning_rate: 5.0000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.0373 - mae: 0.1412 - mse: 0.0370 - val_loss: 0.1103 - val_mae: 0.3505 - val_mse: 0.1894 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0339 - mae: 0.1328 - mse: 0.0330 - val_loss: 0.3139 - val_mae: 0.6667 - val_mse: 0.6079 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.0457 - mae: 0.1484 - mse: 0.0457 - val_loss: 0.0959 - val_mae: 0.3407 - val_mse: 0.1578 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0356 - mae: 0.1268 - mse: 0.0311 - val_loss: 0.1022 - val_mae: 0.3311 - val_mse: 0.1678 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0318 - mae: 0.1242 - mse: 0.0286 - val_loss: 0.1042 - val_mae: 0.3334 - val_mse: 0.1626 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - loss: 0.0360 - mae: 0.1309 - mse: 0.0320 - val_loss: 0.0982 - val_mae: 0.3324 - val_mse: 0.1542 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0381 - mae: 0.1347 - mse: 0.0343 - val_loss: 0.0875 - val_mae: 0.3115 - val_mse: 0.1396 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0362 - mae: 0.1366 - mse: 0.0348 - val_loss: 0.0955 - val_mae: 0.3333 - val_mse: 0.1544 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - loss: 0.0341 - mae: 0.1299 - mse: 0.0326 - val_loss: 0.1935 - val_mae: 0.4651 - val_mse: 0.3456 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0357 - mae: 0.1298 - mse: 0.0330 - val_loss: 0.1027 - val_mae: 0.3432 - val_mse: 0.1627 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0353 - mae: 0.1291 - mse: 0.0314 - val_loss: 0.1597 - val_mae: 0.4208 - val_mse: 0.2857 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0329 - mae: 0.1283 - mse: 0.0307 - val_loss: 0.1242 - val_mae: 0.3659 - val_mse: 0.2179 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 81ms/step - loss: 0.0286 - mae: 0.1232 - mse: 0.0274 - val_loss: 0.1186 - val_mae: 0.3622 - val_mse: 0.2086 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 80ms/step - loss: 0.0300 - mae: 0.1220 - mse: 0.0276 - val_loss: 0.1224 - val_mae: 0.3689 - val_mse: 0.2149 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0291 - mae: 0.1264 - mse: 0.0285 - val_loss: 0.1030 - val_mae: 0.3357 - val_mse: 0.1704 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0313 - mae: 0.1258 - mse: 0.0297 - val_loss: 0.0912 - val_mae: 0.3234 - val_mse: 0.1484 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0328 - mae: 0.1278 - mse: 0.0309\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 83ms/step - loss: 0.0328 - mae: 0.1278 - mse: 0.0309 - val_loss: 0.1139 - val_mae: 0.3518 - val_mse: 0.2016 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0238 - mae: 0.1101 - mse: 0.0240 - val_loss: 0.1012 - val_mae: 0.3392 - val_mse: 0.1811 - learning_rate: 2.5000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 80ms/step - loss: 0.0214 - mae: 0.1090 - mse: 0.0222 - val_loss: 0.0841 - val_mae: 0.3059 - val_mse: 0.1454 - learning_rate: 2.5000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 81ms/step - loss: 0.0222 - mae: 0.1118 - mse: 0.0235 - val_loss: 0.1286 - val_mae: 0.3842 - val_mse: 0.2383 - learning_rate: 2.5000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 81ms/step - loss: 0.0221 - mae: 0.1109 - mse: 0.0237 - val_loss: 0.0993 - val_mae: 0.3402 - val_mse: 0.1781 - learning_rate: 2.5000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 83ms/step - loss: 0.0222 - mae: 0.1110 - mse: 0.0236 - val_loss: 0.1139 - val_mae: 0.3478 - val_mse: 0.2088 - learning_rate: 2.5000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 83ms/step - loss: 0.0202 - mae: 0.1048 - mse: 0.0211 - val_loss: 0.0968 - val_mae: 0.3285 - val_mse: 0.1731 - learning_rate: 2.5000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 81ms/step - loss: 0.0215 - mae: 0.1076 - mse: 0.0225 - val_loss: 0.0907 - val_mae: 0.3089 - val_mse: 0.1612 - learning_rate: 2.5000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0213 - mae: 0.1092 - mse: 0.0226 - val_loss: 0.0964 - val_mae: 0.3313 - val_mse: 0.1746 - learning_rate: 2.5000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0213 - mae: 0.1124 - mse: 0.0233 - val_loss: 0.1052 - val_mae: 0.3417 - val_mse: 0.1927 - learning_rate: 2.5000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 81ms/step - loss: 0.0235 - mae: 0.1141 - mse: 0.0265 - val_loss: 0.0632 - val_mae: 0.2697 - val_mse: 0.1058 - learning_rate: 2.5000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 81ms/step - loss: 0.0229 - mae: 0.1109 - mse: 0.0248 - val_loss: 0.0935 - val_mae: 0.3290 - val_mse: 0.1695 - learning_rate: 2.5000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0201 - mae: 0.1077 - mse: 0.0215 - val_loss: 0.0857 - val_mae: 0.3179 - val_mse: 0.1524 - learning_rate: 2.5000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 83ms/step - loss: 0.0215 - mae: 0.1111 - mse: 0.0238 - val_loss: 0.0986 - val_mae: 0.3333 - val_mse: 0.1771 - learning_rate: 2.5000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0202 - mae: 0.1077 - mse: 0.0209 - val_loss: 0.0801 - val_mae: 0.3012 - val_mse: 0.1396 - learning_rate: 2.5000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 81ms/step - loss: 0.0206 - mae: 0.1090 - mse: 0.0219 - val_loss: 0.0861 - val_mae: 0.3039 - val_mse: 0.1519 - learning_rate: 2.5000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 81ms/step - loss: 0.0192 - mae: 0.1038 - mse: 0.0197 - val_loss: 0.0943 - val_mae: 0.3027 - val_mse: 0.1716 - learning_rate: 2.5000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 81ms/step - loss: 0.0185 - mae: 0.1003 - mse: 0.0190 - val_loss: 0.0963 - val_mae: 0.3197 - val_mse: 0.1757 - learning_rate: 2.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 81ms/step - loss: 0.0185 - mae: 0.1018 - mse: 0.0191 - val_loss: 0.0708 - val_mae: 0.2757 - val_mse: 0.1169 - learning_rate: 2.5000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0219 - mae: 0.1079 - mse: 0.0217 - val_loss: 0.0989 - val_mae: 0.3277 - val_mse: 0.1778 - learning_rate: 2.5000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0215 - mae: 0.1103 - mse: 0.0225\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 83ms/step - loss: 0.0215 - mae: 0.1103 - mse: 0.0225 - val_loss: 0.1212 - val_mae: 0.3613 - val_mse: 0.2258 - learning_rate: 2.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0179 - mae: 0.1021 - mse: 0.0197 - val_loss: 0.0798 - val_mae: 0.2946 - val_mse: 0.1443 - learning_rate: 1.2500e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0171 - mae: 0.0985 - mse: 0.0185 - val_loss: 0.0928 - val_mae: 0.3072 - val_mse: 0.1700 - learning_rate: 1.2500e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 81ms/step - loss: 0.0167 - mae: 0.0993 - mse: 0.0179 - val_loss: 0.0846 - val_mae: 0.2997 - val_mse: 0.1537 - learning_rate: 1.2500e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0178 - mae: 0.0976 - mse: 0.0193 - val_loss: 0.1053 - val_mae: 0.3327 - val_mse: 0.1961 - learning_rate: 1.2500e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 83ms/step - loss: 0.0155 - mae: 0.0927 - mse: 0.0162 - val_loss: 0.0545 - val_mae: 0.2262 - val_mse: 0.0930 - learning_rate: 1.2500e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 83ms/step - loss: 0.0164 - mae: 0.0965 - mse: 0.0178 - val_loss: 0.0768 - val_mae: 0.2669 - val_mse: 0.1381 - learning_rate: 1.2500e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 81ms/step - loss: 0.0161 - mae: 0.0953 - mse: 0.0167 - val_loss: 0.0683 - val_mae: 0.2540 - val_mse: 0.1201 - learning_rate: 1.2500e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 81ms/step - loss: 0.0162 - mae: 0.0951 - mse: 0.0172 - val_loss: 0.0896 - val_mae: 0.2952 - val_mse: 0.1651 - learning_rate: 1.2500e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 81ms/step - loss: 0.0161 - mae: 0.0937 - mse: 0.0171 - val_loss: 0.0577 - val_mae: 0.2395 - val_mse: 0.0998 - learning_rate: 1.2500e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0166 - mae: 0.0969 - mse: 0.0173 - val_loss: 0.0937 - val_mae: 0.3136 - val_mse: 0.1735 - learning_rate: 1.2500e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 81ms/step - loss: 0.0158 - mae: 0.0954 - mse: 0.0169 - val_loss: 0.0767 - val_mae: 0.2840 - val_mse: 0.1385 - learning_rate: 1.2500e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 81ms/step - loss: 0.0154 - mae: 0.0929 - mse: 0.0161 - val_loss: 0.0642 - val_mae: 0.2591 - val_mse: 0.1145 - learning_rate: 1.2500e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 81ms/step - loss: 0.0158 - mae: 0.0965 - mse: 0.0173 - val_loss: 0.0679 - val_mae: 0.2452 - val_mse: 0.1219 - learning_rate: 1.2500e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 81ms/step - loss: 0.0158 - mae: 0.0950 - mse: 0.0171 - val_loss: 0.0889 - val_mae: 0.3093 - val_mse: 0.1654 - learning_rate: 1.2500e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0141 - mae: 0.0895 - mse: 0.0147\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 81ms/step - loss: 0.0141 - mae: 0.0895 - mse: 0.0147 - val_loss: 0.0898 - val_mae: 0.2992 - val_mse: 0.1658 - learning_rate: 1.2500e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 81ms/step - loss: 0.0150 - mae: 0.0928 - mse: 0.0165 - val_loss: 0.0594 - val_mae: 0.2346 - val_mse: 0.1060 - learning_rate: 6.2500e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0140 - mae: 0.0908 - mse: 0.0152 - val_loss: 0.0571 - val_mae: 0.2333 - val_mse: 0.1017 - learning_rate: 6.2500e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0134 - mae: 0.0897 - mse: 0.0143 - val_loss: 0.0461 - val_mae: 0.2046 - val_mse: 0.0799 - learning_rate: 6.2500e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 83ms/step - loss: 0.0137 - mae: 0.0873 - mse: 0.0150 - val_loss: 0.0540 - val_mae: 0.2279 - val_mse: 0.0949 - learning_rate: 6.2500e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 84ms/step - loss: 0.0138 - mae: 0.0896 - mse: 0.0152 - val_loss: 0.0685 - val_mae: 0.2603 - val_mse: 0.1247 - learning_rate: 6.2500e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0134 - mae: 0.0872 - mse: 0.0146 - val_loss: 0.0696 - val_mae: 0.2573 - val_mse: 0.1271 - learning_rate: 6.2500e-05\n",
      "Epoch 72/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0136 - mae: 0.0893 - mse: 0.0151 - val_loss: 0.0673 - val_mae: 0.2554 - val_mse: 0.1232 - learning_rate: 6.2500e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0127 - mae: 0.0853 - mse: 0.0135 - val_loss: 0.0648 - val_mae: 0.2624 - val_mse: 0.1184 - learning_rate: 6.2500e-05\n",
      "Epoch 74/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0145 - mae: 0.0936 - mse: 0.0168 - val_loss: 0.0563 - val_mae: 0.2220 - val_mse: 0.1008 - learning_rate: 6.2500e-05\n",
      "Epoch 75/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 81ms/step - loss: 0.0131 - mae: 0.0870 - mse: 0.0143 - val_loss: 0.0485 - val_mae: 0.2103 - val_mse: 0.0850 - learning_rate: 6.2500e-05\n",
      "Epoch 76/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 83ms/step - loss: 0.0141 - mae: 0.0925 - mse: 0.0158 - val_loss: 0.0622 - val_mae: 0.2347 - val_mse: 0.1124 - learning_rate: 6.2500e-05\n",
      "Epoch 77/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0138 - mae: 0.0915 - mse: 0.0151 - val_loss: 0.0653 - val_mae: 0.2470 - val_mse: 0.1180 - learning_rate: 6.2500e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.0135 - mae: 0.0887 - mse: 0.0148\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0135 - mae: 0.0887 - mse: 0.0148 - val_loss: 0.0543 - val_mae: 0.2196 - val_mse: 0.0962 - learning_rate: 6.2500e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 83ms/step - loss: 0.0124 - mae: 0.0847 - mse: 0.0130 - val_loss: 0.0625 - val_mae: 0.2463 - val_mse: 0.1136 - learning_rate: 3.1250e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 83ms/step - loss: 0.0123 - mae: 0.0843 - mse: 0.0133 - val_loss: 0.0679 - val_mae: 0.2570 - val_mse: 0.1249 - learning_rate: 3.1250e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0127 - mae: 0.0878 - mse: 0.0144 - val_loss: 0.0610 - val_mae: 0.2399 - val_mse: 0.1112 - learning_rate: 3.1250e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0126 - mae: 0.0846 - mse: 0.0142 - val_loss: 0.0569 - val_mae: 0.2239 - val_mse: 0.1026 - learning_rate: 3.1250e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 83ms/step - loss: 0.0123 - mae: 0.0841 - mse: 0.0134 - val_loss: 0.0631 - val_mae: 0.2475 - val_mse: 0.1153 - learning_rate: 3.1250e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 80ms/step - loss: 0.0121 - mae: 0.0842 - mse: 0.0132 - val_loss: 0.0668 - val_mae: 0.2531 - val_mse: 0.1231 - learning_rate: 3.1250e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0124 - mae: 0.0852 - mse: 0.0139 - val_loss: 0.0606 - val_mae: 0.2332 - val_mse: 0.1102 - learning_rate: 3.1250e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0124 - mae: 0.0838 - mse: 0.0138 - val_loss: 0.0800 - val_mae: 0.2970 - val_mse: 0.1495 - learning_rate: 3.1250e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0122 - mae: 0.0858 - mse: 0.0135 - val_loss: 0.0560 - val_mae: 0.2242 - val_mse: 0.1012 - learning_rate: 3.1250e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0118 - mae: 0.0856 - mse: 0.0130\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0118 - mae: 0.0856 - mse: 0.0130 - val_loss: 0.0624 - val_mae: 0.2491 - val_mse: 0.1136 - learning_rate: 3.1250e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 78ms/step - loss: 0.0118 - mae: 0.0846 - mse: 0.0129 - val_loss: 0.0633 - val_mae: 0.2462 - val_mse: 0.1162 - learning_rate: 1.5625e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 78ms/step - loss: 0.0119 - mae: 0.0846 - mse: 0.0133 - val_loss: 0.0629 - val_mae: 0.2492 - val_mse: 0.1154 - learning_rate: 1.5625e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0116 - mae: 0.0815 - mse: 0.0128 - val_loss: 0.0556 - val_mae: 0.2218 - val_mse: 0.1006 - learning_rate: 1.5625e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 78ms/step - loss: 0.0116 - mae: 0.0834 - mse: 0.0129 - val_loss: 0.0581 - val_mae: 0.2332 - val_mse: 0.1060 - learning_rate: 1.5625e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0115 - mae: 0.0840 - mse: 0.0127 - val_loss: 0.0588 - val_mae: 0.2343 - val_mse: 0.1075 - learning_rate: 1.5625e-05\n",
      "Enhanced model training completed!\n",
      "Evaluating enhanced model...\n",
      "Enhanced Model Evaluation Metrics:\n",
      "Training RMSE: $1.8061\n",
      "Testing RMSE: $1.5553\n",
      "Training MAE: $1.0229\n",
      "Testing MAE: $1.1847\n",
      "Training R²: 0.9786\n",
      "Testing R²: 0.9755\n",
      "Training MAPE: 3.12%\n",
      "Testing MAPE: 3.50%\n",
      "Predicting future prices for 10 days...\n",
      "\n",
      "Future Predictions (Next 10 days):\n",
      "Day 1: $21.08\n",
      "Day 2: $20.29\n",
      "Day 3: $19.53\n",
      "Day 4: $18.92\n",
      "Day 5: $18.44\n",
      "Day 6: $18.01\n",
      "Day 7: $17.65\n",
      "Day 8: $17.35\n",
      "Day 9: $17.10\n",
      "Day 10: $16.91\n",
      "Model saved as INTC_enhanced_model.keras\n",
      "Scaler saved as INTC_scaler.pkl\n",
      "Metadata saved as INTC_metadata.pkl\n",
      "\n",
      "Enhanced training completed for INTC\n",
      "Testing MAE: $1.1847\n",
      "Model files saved and ready for use!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential, load_model #type: ignore\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, GRU, Bidirectional #type: ignore\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop #type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau #type: ignore\n",
    "from tensorflow.keras.regularizers import l1_l2 #type: ignore\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedStockPredictor:\n",
    "    \n",
    "    def __init__(self, ticker, api_key, sequence_length=120, test_size=0.15):\n",
    "        self.ticker = ticker\n",
    "        self.api_key = api_key\n",
    "        self.data = None\n",
    "        self.sequence_length = sequence_length  # Increased for more context\n",
    "        self.test_size = test_size  # Reduced for more training data\n",
    "        self.scaler = RobustScaler()  # More robust to outliers\n",
    "        self.model = None\n",
    "        self.scaled_data = None\n",
    "        self.features = None\n",
    "\n",
    "    def fetch_data(self, period=\"5y\"):\n",
    "        \"\"\"Enhanced data fetching with more features\"\"\"\n",
    "        print(f\"Attempting to fetch data for {self.ticker} using yfinance...\")\n",
    "\n",
    "        try:\n",
    "            self.data = yf.download(self.ticker, period=period, threads=False)\n",
    "            if self.data.empty:\n",
    "                raise ValueError(\"yfinance returned empty data.\")\n",
    "            print(\"Data fetched from yfinance.\")\n",
    "        except Exception as e:\n",
    "            print(f\"yfinance failed: {e}\")\n",
    "            print(f\"Falling back to Alpha Vantage for {self.ticker}...\")\n",
    "            self.data = self._fetch_from_alpha_vantage(period)\n",
    "            print(\"Data fetched from Alpha Vantage.\")\n",
    "\n",
    "        # Enhanced technical indicators\n",
    "        self.data['MA_5'] = self.data['Close'].rolling(window=5).mean()\n",
    "        self.data['MA_10'] = self.data['Close'].rolling(window=10).mean()\n",
    "        self.data['MA_20'] = self.data['Close'].rolling(window=20).mean()\n",
    "        self.data['MA_50'] = self.data['Close'].rolling(window=50).mean()\n",
    "        self.data['MA_200'] = self.data['Close'].rolling(window=200).mean()\n",
    "        \n",
    "        # More technical indicators\n",
    "        self.data['RSI'] = self._calculate_rsi(self.data['Close'])\n",
    "        self.data['MACD'], self.data['MACD_signal'] = self._calculate_macd(self.data['Close'])\n",
    "        self.data['BB_upper'], self.data['BB_lower'] = self._calculate_bollinger_bands(self.data['Close'])\n",
    "        self.data['ATR'] = self._calculate_atr(self.data['High'], self.data['Low'], self.data['Close'])\n",
    "        \n",
    "        # Price-based features\n",
    "        self.data['Price_Change'] = self.data['Close'].pct_change()\n",
    "        self.data['High_Low_Ratio'] = self.data['High'] / self.data['Low']\n",
    "        self.data['Volume_MA'] = self.data['Volume'].rolling(window=20).mean()\n",
    "        self.data['Volume_Ratio'] = self.data['Volume'] / self.data['Volume_MA']\n",
    "        \n",
    "        # Lag features\n",
    "        for lag in [1, 2, 3, 5]:\n",
    "            self.data[f'Close_lag_{lag}'] = self.data['Close'].shift(lag)\n",
    "            self.data[f'Volume_lag_{lag}'] = self.data['Volume'].shift(lag)\n",
    "        \n",
    "        # Clean up\n",
    "        self.data = self.data.dropna()\n",
    "        print(f\"Final data shape: {self.data.shape}\")\n",
    "\n",
    "    def _fetch_from_alpha_vantage(self, period):\n",
    "        ts = TimeSeries(key=self.api_key, output_format='pandas')\n",
    "        outputsize = 'full' if period in ['2y', '5y', 'max'] else 'compact'\n",
    "        \n",
    "        try:\n",
    "            data, _ = ts.get_daily(symbol=self.ticker, outputsize=outputsize)\n",
    "            data.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "            data = data.sort_index()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Alpha Vantage failed: {e}\")\n",
    "        \n",
    "        if data.empty:\n",
    "            raise ValueError(f\"No data found for {self.ticker} from Alpha Vantage.\")\n",
    "        return data\n",
    "\n",
    "    def _calculate_rsi(self, series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "        loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    def _calculate_macd(self, series, fast=12, slow=26, signal=9):\n",
    "        ema_fast = series.ewm(span=fast).mean()\n",
    "        ema_slow = series.ewm(span=slow).mean()\n",
    "        macd = ema_fast - ema_slow\n",
    "        macd_signal = macd.ewm(span=signal).mean()\n",
    "        return macd, macd_signal\n",
    "    \n",
    "    def _calculate_bollinger_bands(self, series, window=20, std_dev=2):\n",
    "        rolling_mean = series.rolling(window=window).mean()\n",
    "        rolling_std = series.rolling(window=window).std()\n",
    "        upper_band = rolling_mean + (rolling_std * std_dev)\n",
    "        lower_band = rolling_mean - (rolling_std * std_dev)\n",
    "        return upper_band, lower_band\n",
    "    \n",
    "    def _calculate_atr(self, high, low, close, period=14):\n",
    "        high_low = high - low\n",
    "        high_close = np.abs(high - close.shift())\n",
    "        low_close = np.abs(low - close.shift())\n",
    "        ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "        true_range = np.max(ranges, axis=1)\n",
    "        return true_range.rolling(period).mean()\n",
    "    \n",
    "    def prepare_data(self, features=None):\n",
    "        \"\"\"Enhanced data preparation with more features\"\"\"\n",
    "        if features is None:\n",
    "            features = [\n",
    "                'Close', 'Open', 'High', 'Low', 'Volume',\n",
    "                'MA_5', 'MA_10', 'MA_20', 'MA_50', 'MA_200',\n",
    "                'RSI', 'MACD', 'MACD_signal', 'BB_upper', 'BB_lower', 'ATR',\n",
    "                'Price_Change', 'High_Low_Ratio', 'Volume_Ratio',\n",
    "                'Close_lag_1', 'Close_lag_2', 'Close_lag_3', 'Close_lag_5',\n",
    "                'Volume_lag_1', 'Volume_lag_2', 'Volume_lag_3', 'Volume_lag_5'\n",
    "            ]\n",
    "        \n",
    "        print(\"Preparing enhanced data...\")\n",
    "        \n",
    "        # Select available features (some might not exist if data is limited)\n",
    "        available_features = [f for f in features if f in self.data.columns]\n",
    "        print(f\"Using {len(available_features)} features: {available_features}\")\n",
    "        \n",
    "        # Store features for later use\n",
    "        self.features = available_features\n",
    "        \n",
    "        # Select features\n",
    "        feature_data = self.data[available_features].values\n",
    "        \n",
    "        # Scale the data\n",
    "        self.scaled_data = self.scaler.fit_transform(feature_data)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(self.sequence_length, len(self.scaled_data)):\n",
    "            X.append(self.scaled_data[i-self.sequence_length:i])\n",
    "            y.append(self.scaled_data[i, 0])  # Predict 'Close' price (first feature)\n",
    "            \n",
    "        X, y = np.array(X), np.array(y)\n",
    "        \n",
    "        # Split data\n",
    "        split_idx = int(len(X) * (1 - self.test_size))\n",
    "        self.X_train = X[:split_idx]\n",
    "        self.X_test = X[split_idx:]\n",
    "        self.y_train = y[:split_idx]\n",
    "        self.y_test = y[split_idx:]\n",
    "        \n",
    "        print(f\"Training data shape: X={self.X_train.shape}, y={self.y_train.shape}\")\n",
    "        print(f\"Testing data shape: X={self.X_test.shape}, y={self.y_test.shape}\")\n",
    "        \n",
    "    def build_enhanced_model(self, model_type='lstm_advanced'):\n",
    "        \"\"\"Build enhanced model architectures\"\"\"\n",
    "        print(f\"Building enhanced model: {model_type}\")\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        \n",
    "        if model_type == 'lstm_advanced':\n",
    "            # Advanced LSTM with Bidirectional layers\n",
    "            self.model.add(Bidirectional(LSTM(128, return_sequences=True), \n",
    "                                       input_shape=(self.X_train.shape[1], self.X_train.shape[2])))\n",
    "            self.model.add(BatchNormalization())\n",
    "            self.model.add(Dropout(0.2))\n",
    "            \n",
    "            self.model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "            self.model.add(BatchNormalization())\n",
    "            self.model.add(Dropout(0.2))\n",
    "            \n",
    "            self.model.add(LSTM(32, return_sequences=False))\n",
    "            self.model.add(BatchNormalization())\n",
    "            self.model.add(Dropout(0.2))\n",
    "            \n",
    "        elif model_type == 'gru_ensemble':\n",
    "            # GRU-based architecture\n",
    "            self.model.add(GRU(128, return_sequences=True, \n",
    "                              input_shape=(self.X_train.shape[1], self.X_train.shape[2])))\n",
    "            self.model.add(BatchNormalization())\n",
    "            self.model.add(Dropout(0.3))\n",
    "            \n",
    "            self.model.add(GRU(64, return_sequences=True))\n",
    "            self.model.add(BatchNormalization())\n",
    "            self.model.add(Dropout(0.3))\n",
    "            \n",
    "            self.model.add(GRU(32))\n",
    "            self.model.add(BatchNormalization())\n",
    "            self.model.add(Dropout(0.3))\n",
    "        \n",
    "        # Dense layers with regularization\n",
    "        self.model.add(Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.3))\n",
    "        \n",
    "        self.model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.2))\n",
    "        \n",
    "        self.model.add(Dense(16, activation='relu'))\n",
    "        self.model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "        # Compile with different optimizers and loss functions\n",
    "        self.model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999),\n",
    "            loss='huber',  # More robust to outliers than MSE\n",
    "            metrics=['mae', 'mse']\n",
    "        )\n",
    "        \n",
    "        print(\"Enhanced model built successfully!\")\n",
    "        print(self.model.summary())\n",
    "        \n",
    "    def train_enhanced_model(self, epochs=200, batch_size=16, validation_split=0.15):\n",
    "        \"\"\"Enhanced training with better callbacks\"\"\"\n",
    "        print(\"Training enhanced model...\")\n",
    "        \n",
    "        # Enhanced callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=25,  # Increased patience\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-6\n",
    "        )\n",
    "        \n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            f'{self.ticker}_best_enhanced_model.keras', \n",
    "            monitor='val_loss', \n",
    "            save_best_only=True,\n",
    "            save_weights_only=False\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "            verbose=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(\"Enhanced model training completed!\")\n",
    "        return history\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Enhanced evaluation with more metrics\"\"\"\n",
    "        print(\"Evaluating enhanced model...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        train_predictions = self.model.predict(self.X_train, verbose=0)\n",
    "        test_predictions = self.model.predict(self.X_test, verbose=0)\n",
    "        \n",
    "        # Inverse transform predictions (more robust method)\n",
    "        def inverse_transform_predictions(predictions, original_shape):\n",
    "            # Create dummy array with same shape as original features\n",
    "            dummy_features = np.zeros((len(predictions), original_shape))\n",
    "            dummy_features[:, 0] = predictions.flatten()\n",
    "            # Inverse transform and return only the first column (Close price)\n",
    "            return self.scaler.inverse_transform(dummy_features)[:, 0]\n",
    "        \n",
    "        train_pred_original = inverse_transform_predictions(train_predictions, self.scaled_data.shape[1])\n",
    "        test_pred_original = inverse_transform_predictions(test_predictions, self.scaled_data.shape[1])\n",
    "        \n",
    "        # Inverse transform actual values\n",
    "        train_actual_original = inverse_transform_predictions(self.y_train.reshape(-1, 1), self.scaled_data.shape[1])\n",
    "        test_actual_original = inverse_transform_predictions(self.y_test.reshape(-1, 1), self.scaled_data.shape[1])\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        train_rmse = np.sqrt(mean_squared_error(train_actual_original, train_pred_original))\n",
    "        test_rmse = np.sqrt(mean_squared_error(test_actual_original, test_pred_original))\n",
    "        train_mae = mean_absolute_error(train_actual_original, train_pred_original)\n",
    "        test_mae = mean_absolute_error(test_actual_original, test_pred_original)\n",
    "        train_r2 = r2_score(train_actual_original, train_pred_original)\n",
    "        test_r2 = r2_score(test_actual_original, test_pred_original)\n",
    "        \n",
    "        # Additional metrics\n",
    "        train_mape = np.mean(np.abs((train_actual_original - train_pred_original) / train_actual_original)) * 100\n",
    "        test_mape = np.mean(np.abs((test_actual_original - test_pred_original) / test_actual_original)) * 100\n",
    "        \n",
    "        metrics = {\n",
    "            'train_rmse': train_rmse,\n",
    "            'test_rmse': test_rmse,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'train_mape': train_mape,\n",
    "            'test_mape': test_mape\n",
    "        }\n",
    "        \n",
    "        print(\"Enhanced Model Evaluation Metrics:\")\n",
    "        print(f\"Training RMSE: ${train_rmse:.4f}\")\n",
    "        print(f\"Testing RMSE: ${test_rmse:.4f}\")\n",
    "        print(f\"Training MAE: ${train_mae:.4f}\")\n",
    "        print(f\"Testing MAE: ${test_mae:.4f}\")\n",
    "        print(f\"Training R²: {train_r2:.4f}\")\n",
    "        print(f\"Testing R²: {test_r2:.4f}\")\n",
    "        print(f\"Training MAPE: {train_mape:.2f}%\")\n",
    "        print(f\"Testing MAPE: {test_mape:.2f}%\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def predict_future(self, days=10):\n",
    "        \"\"\"\n",
    "        Predict future stock prices for the specified number of days\n",
    "        \"\"\"\n",
    "        print(f\"Predicting future prices for {days} days...\")\n",
    "        \n",
    "        # Get the last sequence_length days of scaled data\n",
    "        last_sequence = self.scaled_data[-self.sequence_length:].copy()\n",
    "        future_predictions = []\n",
    "        \n",
    "        for day in range(days):\n",
    "            # Reshape for prediction\n",
    "            current_sequence = last_sequence.reshape(1, self.sequence_length, len(self.features))\n",
    "            \n",
    "            # Make prediction\n",
    "            next_pred = self.model.predict(current_sequence, verbose=0)[0, 0]\n",
    "            future_predictions.append(next_pred)\n",
    "            \n",
    "            # Update the sequence for next prediction\n",
    "            # Create new row with predicted price and estimated other features\n",
    "            new_row = last_sequence[-1].copy()  # Copy last row\n",
    "            new_row[0] = next_pred  # Update the Close price (first feature)\n",
    "            \n",
    "            # For simplicity, we'll use the last known values for other features\n",
    "            # In a more sophisticated approach, you might predict these as well\n",
    "            \n",
    "            # Shift the sequence and add the new prediction\n",
    "            last_sequence = np.roll(last_sequence, -1, axis=0)\n",
    "            last_sequence[-1] = new_row\n",
    "        \n",
    "        # Inverse transform predictions to get actual prices\n",
    "        future_predictions = np.array(future_predictions).reshape(-1, 1)\n",
    "        \n",
    "        # Create dummy array for inverse transform\n",
    "        dummy_features = np.zeros((len(future_predictions), len(self.features)))\n",
    "        dummy_features[:, 0] = future_predictions.flatten()\n",
    "        \n",
    "        # Inverse transform to get actual prices\n",
    "        future_prices = self.scaler.inverse_transform(dummy_features)[:, 0]\n",
    "        \n",
    "        return future_prices\n",
    "    \n",
    "    def save_model(self):\n",
    "        \"\"\"Save the trained model and scaler\"\"\"\n",
    "        model_filename = f'{self.ticker}_enhanced_model.keras'\n",
    "        scaler_filename = f'{self.ticker}_scaler.pkl'\n",
    "        \n",
    "        # Save model\n",
    "        self.model.save(model_filename)\n",
    "        print(f\"Model saved as {model_filename}\")\n",
    "        \n",
    "        # Save scaler\n",
    "        joblib.dump(self.scaler, scaler_filename)\n",
    "        print(f\"Scaler saved as {scaler_filename}\")\n",
    "        \n",
    "        # Save additional metadata\n",
    "        metadata = {\n",
    "            'features': self.features,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'ticker': self.ticker\n",
    "        }\n",
    "        \n",
    "        metadata_filename = f'{self.ticker}_metadata.pkl'\n",
    "        joblib.dump(metadata, metadata_filename)\n",
    "        print(f\"Metadata saved as {metadata_filename}\")\n",
    "    \n",
    "    def load_model(self, model_filename=None, scaler_filename=None, metadata_filename=None):\n",
    "        \"\"\"Load a previously trained model\"\"\"\n",
    "        if model_filename is None:\n",
    "            model_filename = f'{self.ticker}_enhanced_model.keras'\n",
    "        if scaler_filename is None:\n",
    "            scaler_filename = f'{self.ticker}_scaler.pkl'\n",
    "        if metadata_filename is None:\n",
    "            metadata_filename = f'{self.ticker}_metadata.pkl'\n",
    "        \n",
    "        # Load model\n",
    "        self.model = load_model(model_filename)\n",
    "        print(f\"Model loaded from {model_filename}\")\n",
    "        \n",
    "        # Load scaler\n",
    "        self.scaler = joblib.load(scaler_filename)\n",
    "        print(f\"Scaler loaded from {scaler_filename}\")\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata = joblib.load(metadata_filename)\n",
    "        self.features = metadata['features']\n",
    "        self.sequence_length = metadata['sequence_length']\n",
    "        print(f\"Metadata loaded from {metadata_filename}\")\n",
    "\n",
    "# Enhanced training function\n",
    "def train_enhanced_stock_model(ticker_symbol, api_key, model_type='lstm_advanced', save_model=True):\n",
    "    \"\"\"\n",
    "    Enhanced training pipeline for better MAE\n",
    "    \"\"\"\n",
    "    # Initialize enhanced predictor\n",
    "    predictor = EnhancedStockPredictor(ticker_symbol, api_key, sequence_length=120, test_size=0.15)\n",
    "    \n",
    "    # Fetch and prepare enhanced data\n",
    "    predictor.fetch_data(period=\"5y\")  # More data\n",
    "    predictor.prepare_data()\n",
    "    \n",
    "    # Build and train enhanced model\n",
    "    predictor.build_enhanced_model(model_type=model_type)\n",
    "    history = predictor.train_enhanced_model(epochs=200, batch_size=16)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = predictor.evaluate_model()\n",
    "    \n",
    "    # Make future predictions\n",
    "    future_predictions = predictor.predict_future(days=10)\n",
    "    \n",
    "    print(\"\\nFuture Predictions (Next 10 days):\")\n",
    "    for i, pred in enumerate(future_predictions, 1):\n",
    "        print(f\"Day {i}: ${pred:.2f}\")\n",
    "    \n",
    "    # Save model if requested\n",
    "    if save_model:\n",
    "        predictor.save_model()\n",
    "    \n",
    "    return predictor, history, metrics, future_predictions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your API key\n",
    "    API_KEY = \"JO6S4PKBXUZMZWE1\"\n",
    "    \n",
    "    # Example: Train enhanced model for stock\n",
    "    #ticker = [\"GOOGL\", \"MSFT\", \"NVDA\", \"AMD\", \"INTC\"]\n",
    "    ticker = \"INTC\"\n",
    "    print(\"Training Enhanced Stock Predictor...\")\n",
    "    print(\"This will take longer but should achieve better MAE!\")\n",
    "    \n",
    "    predictor, history, metrics, predictions = train_enhanced_stock_model(\n",
    "        ticker, API_KEY, model_type='lstm_advanced', save_model=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEnhanced training completed for {ticker}\")\n",
    "    print(f\"Testing MAE: ${metrics['test_mae']:.4f}\")\n",
    "    print(\"Model files saved and ready for use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

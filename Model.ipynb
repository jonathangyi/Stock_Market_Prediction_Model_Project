{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Enhanced Stock Predictor...\n",
      "This will take longer but should achieve better MAE!\n",
      "Attempting to fetch data for AAPL using yfinance...\n",
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['AAPL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yfinance failed: yfinance returned empty data.\n",
      "Falling back to Alpha Vantage for AAPL...\n",
      "Data fetched from Alpha Vantage.\n",
      "Final data shape: (6242, 28)\n",
      "Preparing enhanced data...\n",
      "Using 27 features: ['Close', 'Open', 'High', 'Low', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'MA_50', 'MA_200', 'RSI', 'MACD', 'MACD_signal', 'BB_upper', 'BB_lower', 'ATR', 'Price_Change', 'High_Low_Ratio', 'Volume_Ratio', 'Close_lag_1', 'Close_lag_2', 'Close_lag_3', 'Close_lag_5', 'Volume_lag_1', 'Volume_lag_2', 'Volume_lag_3', 'Volume_lag_5']\n",
      "Training data shape: X=(5203, 120, 27), y=(5203,)\n",
      "Testing data shape: X=(919, 120, 27), y=(919,)\n",
      "Building enhanced model: lstm_advanced\n",
      "Enhanced model built successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">159,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,608</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m159,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m164,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m20,608\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">351,489</span> (1.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m351,489\u001b[0m (1.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">350,465</span> (1.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m350,465\u001b[0m (1.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> (4.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,024\u001b[0m (4.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Training enhanced model...\n",
      "Epoch 1/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 85ms/step - loss: 6.0664 - mae: 0.8787 - mse: 1.3319 - val_loss: 4.3259 - val_mae: 0.3639 - val_mse: 0.1943 - learning_rate: 5.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 3.9901 - mae: 0.4887 - mse: 0.4473 - val_loss: 2.6178 - val_mae: 0.2916 - val_mse: 0.1258 - learning_rate: 5.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 2.3456 - mae: 0.4004 - mse: 0.3102 - val_loss: 1.3577 - val_mae: 0.2847 - val_mse: 0.1505 - learning_rate: 5.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 1.1629 - mae: 0.3378 - mse: 0.2286 - val_loss: 0.5848 - val_mae: 0.2610 - val_mse: 0.1119 - learning_rate: 5.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.5151 - mae: 0.3036 - mse: 0.1863 - val_loss: 0.2701 - val_mae: 0.2423 - val_mse: 0.1232 - learning_rate: 5.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.2541 - mae: 0.2830 - mse: 0.1606 - val_loss: 0.1635 - val_mae: 0.2235 - val_mse: 0.0975 - learning_rate: 5.0000e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.1700 - mae: 0.2623 - mse: 0.1382 - val_loss: 0.1179 - val_mae: 0.2244 - val_mse: 0.0881 - learning_rate: 5.0000e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.1469 - mae: 0.2687 - mse: 0.1492 - val_loss: 0.0836 - val_mae: 0.1450 - val_mse: 0.0419 - learning_rate: 5.0000e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 75ms/step - loss: 0.1285 - mae: 0.2510 - mse: 0.1326 - val_loss: 0.0712 - val_mae: 0.1297 - val_mse: 0.0350 - learning_rate: 5.0000e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 75ms/step - loss: 0.1141 - mae: 0.2444 - mse: 0.1206 - val_loss: 0.0834 - val_mae: 0.1653 - val_mse: 0.0658 - learning_rate: 5.0000e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.1015 - mae: 0.2253 - mse: 0.1029 - val_loss: 0.0720 - val_mae: 0.1338 - val_mse: 0.0361 - learning_rate: 5.0000e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 80ms/step - loss: 0.1062 - mae: 0.2287 - mse: 0.1081 - val_loss: 0.0792 - val_mae: 0.1446 - val_mse: 0.0433 - learning_rate: 5.0000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.1091 - mae: 0.2287 - mse: 0.1080 - val_loss: 0.0644 - val_mae: 0.1085 - val_mse: 0.0274 - learning_rate: 5.0000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.1074 - mae: 0.2357 - mse: 0.1118 - val_loss: 0.0644 - val_mae: 0.1124 - val_mse: 0.0359 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0986 - mae: 0.2200 - mse: 0.0976 - val_loss: 0.0634 - val_mae: 0.1199 - val_mse: 0.0372 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.1074 - mae: 0.2249 - mse: 0.1098 - val_loss: 0.0766 - val_mae: 0.1859 - val_mse: 0.0668 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0948 - mae: 0.2124 - mse: 0.0958 - val_loss: 0.0797 - val_mae: 0.2043 - val_mse: 0.0716 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0971 - mae: 0.2175 - mse: 0.1008 - val_loss: 0.0840 - val_mae: 0.1657 - val_mse: 0.0649 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 75ms/step - loss: 0.0966 - mae: 0.2114 - mse: 0.0919 - val_loss: 0.0570 - val_mae: 0.1368 - val_mse: 0.0398 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0998 - mae: 0.2268 - mse: 0.1089 - val_loss: 0.0552 - val_mae: 0.1039 - val_mse: 0.0297 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0931 - mae: 0.2175 - mse: 0.0997 - val_loss: 0.0497 - val_mae: 0.0960 - val_mse: 0.0239 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0862 - mae: 0.2108 - mse: 0.0987 - val_loss: 0.0649 - val_mae: 0.1399 - val_mse: 0.0455 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 78ms/step - loss: 0.0902 - mae: 0.2175 - mse: 0.0965 - val_loss: 0.0565 - val_mae: 0.1166 - val_mse: 0.0355 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0850 - mae: 0.2091 - mse: 0.0945 - val_loss: 0.0603 - val_mae: 0.1414 - val_mse: 0.0436 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0859 - mae: 0.2070 - mse: 0.0894 - val_loss: 0.0813 - val_mae: 0.1775 - val_mse: 0.0832 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0831 - mae: 0.2036 - mse: 0.0991 - val_loss: 0.0605 - val_mae: 0.1701 - val_mse: 0.0481 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0815 - mae: 0.2060 - mse: 0.0857 - val_loss: 0.0515 - val_mae: 0.1230 - val_mse: 0.0355 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0816 - mae: 0.2062 - mse: 0.0872 - val_loss: 0.0579 - val_mae: 0.1483 - val_mse: 0.0503 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0864 - mae: 0.2218 - mse: 0.0970 - val_loss: 0.0494 - val_mae: 0.1103 - val_mse: 0.0274 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0774 - mae: 0.1995 - mse: 0.0885 - val_loss: 0.0547 - val_mae: 0.1035 - val_mse: 0.0282 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0844 - mae: 0.2086 - mse: 0.0850 - val_loss: 0.0568 - val_mae: 0.1239 - val_mse: 0.0327 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0809 - mae: 0.2002 - mse: 0.0820 - val_loss: 0.0791 - val_mae: 0.2373 - val_mse: 0.0743 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0891 - mae: 0.2115 - mse: 0.0913 - val_loss: 0.0676 - val_mae: 0.1691 - val_mse: 0.0578 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 78ms/step - loss: 0.0956 - mae: 0.2221 - mse: 0.1022 - val_loss: 0.0737 - val_mae: 0.2018 - val_mse: 0.0705 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 96ms/step - loss: 0.0810 - mae: 0.2107 - mse: 0.0864 - val_loss: 0.0434 - val_mae: 0.1040 - val_mse: 0.0298 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 92ms/step - loss: 0.0758 - mae: 0.2014 - mse: 0.0855 - val_loss: 0.0533 - val_mae: 0.1225 - val_mse: 0.0393 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 95ms/step - loss: 0.0745 - mae: 0.1990 - mse: 0.0775 - val_loss: 0.0729 - val_mae: 0.1573 - val_mse: 0.0502 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 92ms/step - loss: 0.0809 - mae: 0.2033 - mse: 0.0785 - val_loss: 0.0529 - val_mae: 0.1194 - val_mse: 0.0324 - learning_rate: 5.0000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0718 - mae: 0.1920 - mse: 0.0733 - val_loss: 0.0506 - val_mae: 0.1374 - val_mse: 0.0366 - learning_rate: 5.0000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 85ms/step - loss: 0.0770 - mae: 0.2019 - mse: 0.0814 - val_loss: 0.0636 - val_mae: 0.1396 - val_mse: 0.0397 - learning_rate: 5.0000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0756 - mae: 0.1948 - mse: 0.0724 - val_loss: 0.0535 - val_mae: 0.1678 - val_mse: 0.0466 - learning_rate: 5.0000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0693 - mae: 0.1902 - mse: 0.0743 - val_loss: 0.0551 - val_mae: 0.1533 - val_mse: 0.0383 - learning_rate: 5.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 0.0770 - mae: 0.1998 - mse: 0.0828 - val_loss: 0.0494 - val_mae: 0.1137 - val_mse: 0.0288 - learning_rate: 5.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - loss: 0.0677 - mae: 0.1860 - mse: 0.0660 - val_loss: 0.0521 - val_mae: 0.1266 - val_mse: 0.0382 - learning_rate: 5.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0769 - mae: 0.2006 - mse: 0.0793\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0769 - mae: 0.2005 - mse: 0.0792 - val_loss: 0.0457 - val_mae: 0.1288 - val_mse: 0.0309 - learning_rate: 5.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 80ms/step - loss: 0.0615 - mae: 0.1798 - mse: 0.0644 - val_loss: 0.0410 - val_mae: 0.1228 - val_mse: 0.0276 - learning_rate: 2.5000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 75ms/step - loss: 0.0525 - mae: 0.1639 - mse: 0.0525 - val_loss: 0.0484 - val_mae: 0.1504 - val_mse: 0.0480 - learning_rate: 2.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0541 - mae: 0.1733 - mse: 0.0569 - val_loss: 0.0455 - val_mae: 0.1558 - val_mse: 0.0428 - learning_rate: 2.5000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0533 - mae: 0.1719 - mse: 0.0572 - val_loss: 0.0339 - val_mae: 0.0933 - val_mse: 0.0217 - learning_rate: 2.5000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0510 - mae: 0.1669 - mse: 0.0538 - val_loss: 0.0345 - val_mae: 0.0800 - val_mse: 0.0213 - learning_rate: 2.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 75ms/step - loss: 0.0501 - mae: 0.1595 - mse: 0.0533 - val_loss: 0.0494 - val_mae: 0.1380 - val_mse: 0.0440 - learning_rate: 2.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0527 - mae: 0.1638 - mse: 0.0541 - val_loss: 0.0335 - val_mae: 0.0858 - val_mse: 0.0225 - learning_rate: 2.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0538 - mae: 0.1702 - mse: 0.0579 - val_loss: 0.0307 - val_mae: 0.0766 - val_mse: 0.0196 - learning_rate: 2.5000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 75ms/step - loss: 0.0478 - mae: 0.1613 - mse: 0.0503 - val_loss: 0.0333 - val_mae: 0.0973 - val_mse: 0.0241 - learning_rate: 2.5000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0527 - mae: 0.1684 - mse: 0.0565 - val_loss: 0.0425 - val_mae: 0.1424 - val_mse: 0.0391 - learning_rate: 2.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 78ms/step - loss: 0.0496 - mae: 0.1603 - mse: 0.0509 - val_loss: 0.0331 - val_mae: 0.0876 - val_mse: 0.0216 - learning_rate: 2.5000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0491 - mae: 0.1567 - mse: 0.0518 - val_loss: 0.0554 - val_mae: 0.1841 - val_mse: 0.0666 - learning_rate: 2.5000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 75ms/step - loss: 0.0536 - mae: 0.1689 - mse: 0.0580 - val_loss: 0.0351 - val_mae: 0.1046 - val_mse: 0.0265 - learning_rate: 2.5000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0502 - mae: 0.1645 - mse: 0.0552 - val_loss: 0.0469 - val_mae: 0.1483 - val_mse: 0.0509 - learning_rate: 2.5000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0496 - mae: 0.1640 - mse: 0.0538 - val_loss: 0.0340 - val_mae: 0.0955 - val_mse: 0.0231 - learning_rate: 2.5000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 75ms/step - loss: 0.0485 - mae: 0.1592 - mse: 0.0513 - val_loss: 0.0389 - val_mae: 0.0747 - val_mse: 0.0194 - learning_rate: 2.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 75ms/step - loss: 0.0587 - mae: 0.1736 - mse: 0.0595 - val_loss: 0.0326 - val_mae: 0.0877 - val_mse: 0.0197 - learning_rate: 2.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0519 - mae: 0.1630 - mse: 0.0618\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 78ms/step - loss: 0.0519 - mae: 0.1630 - mse: 0.0618 - val_loss: 0.0397 - val_mae: 0.1133 - val_mse: 0.0278 - learning_rate: 2.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0461 - mae: 0.1526 - mse: 0.0461 - val_loss: 0.0298 - val_mae: 0.1018 - val_mse: 0.0250 - learning_rate: 1.2500e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 85ms/step - loss: 0.0384 - mae: 0.1418 - mse: 0.0399 - val_loss: 0.0288 - val_mae: 0.0952 - val_mse: 0.0255 - learning_rate: 1.2500e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0389 - mae: 0.1461 - mse: 0.0411 - val_loss: 0.0268 - val_mae: 0.0816 - val_mse: 0.0202 - learning_rate: 1.2500e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 81ms/step - loss: 0.0420 - mae: 0.1478 - mse: 0.0495 - val_loss: 0.0255 - val_mae: 0.0728 - val_mse: 0.0190 - learning_rate: 1.2500e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0364 - mae: 0.1341 - mse: 0.0398 - val_loss: 0.0271 - val_mae: 0.0917 - val_mse: 0.0222 - learning_rate: 1.2500e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0411 - mae: 0.1487 - mse: 0.0463 - val_loss: 0.0255 - val_mae: 0.0894 - val_mse: 0.0214 - learning_rate: 1.2500e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0392 - mae: 0.1469 - mse: 0.0475 - val_loss: 0.0283 - val_mae: 0.1062 - val_mse: 0.0251 - learning_rate: 1.2500e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 84ms/step - loss: 0.0404 - mae: 0.1462 - mse: 0.0479 - val_loss: 0.0242 - val_mae: 0.0663 - val_mse: 0.0171 - learning_rate: 1.2500e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0360 - mae: 0.1372 - mse: 0.0396 - val_loss: 0.0252 - val_mae: 0.0778 - val_mse: 0.0178 - learning_rate: 1.2500e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0436 - mae: 0.1551 - mse: 0.0479 - val_loss: 0.0291 - val_mae: 0.1140 - val_mse: 0.0262 - learning_rate: 1.2500e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0418 - mae: 0.1520 - mse: 0.0476 - val_loss: 0.0335 - val_mae: 0.1216 - val_mse: 0.0339 - learning_rate: 1.2500e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 81ms/step - loss: 0.0387 - mae: 0.1463 - mse: 0.0425 - val_loss: 0.0403 - val_mae: 0.1559 - val_mse: 0.0495 - learning_rate: 1.2500e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0410 - mae: 0.1494 - mse: 0.0459 - val_loss: 0.0252 - val_mae: 0.0882 - val_mse: 0.0217 - learning_rate: 1.2500e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 75ms/step - loss: 0.0401 - mae: 0.1478 - mse: 0.0484 - val_loss: 0.0269 - val_mae: 0.0933 - val_mse: 0.0253 - learning_rate: 1.2500e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0418 - mae: 0.1501 - mse: 0.0526 - val_loss: 0.0246 - val_mae: 0.0804 - val_mse: 0.0191 - learning_rate: 1.2500e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0391 - mae: 0.1474 - mse: 0.0480 - val_loss: 0.0255 - val_mae: 0.0865 - val_mse: 0.0227 - learning_rate: 1.2500e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 78ms/step - loss: 0.0385 - mae: 0.1408 - mse: 0.0558 - val_loss: 0.0367 - val_mae: 0.1715 - val_mse: 0.0431 - learning_rate: 1.2500e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0363 - mae: 0.1391 - mse: 0.0397\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 0.0363 - mae: 0.1391 - mse: 0.0397 - val_loss: 0.0291 - val_mae: 0.1238 - val_mse: 0.0278 - learning_rate: 1.2500e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0351 - mae: 0.1364 - mse: 0.0394 - val_loss: 0.0236 - val_mae: 0.0830 - val_mse: 0.0215 - learning_rate: 6.2500e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0325 - mae: 0.1301 - mse: 0.0362 - val_loss: 0.0248 - val_mae: 0.1079 - val_mse: 0.0238 - learning_rate: 6.2500e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0314 - mae: 0.1317 - mse: 0.0335 - val_loss: 0.0223 - val_mae: 0.0805 - val_mse: 0.0201 - learning_rate: 6.2500e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 0.0329 - mae: 0.1347 - mse: 0.0385 - val_loss: 0.0219 - val_mae: 0.0739 - val_mse: 0.0192 - learning_rate: 6.2500e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0328 - mae: 0.1342 - mse: 0.0366 - val_loss: 0.0225 - val_mae: 0.0791 - val_mse: 0.0210 - learning_rate: 6.2500e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0322 - mae: 0.1301 - mse: 0.0350 - val_loss: 0.0231 - val_mae: 0.0905 - val_mse: 0.0225 - learning_rate: 6.2500e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 0.0334 - mae: 0.1350 - mse: 0.0386 - val_loss: 0.0218 - val_mae: 0.0777 - val_mse: 0.0191 - learning_rate: 6.2500e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0301 - mae: 0.1278 - mse: 0.0319 - val_loss: 0.0225 - val_mae: 0.0753 - val_mse: 0.0185 - learning_rate: 6.2500e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0318 - mae: 0.1298 - mse: 0.0343 - val_loss: 0.0221 - val_mae: 0.0766 - val_mse: 0.0200 - learning_rate: 6.2500e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 0.0309 - mae: 0.1267 - mse: 0.0364 - val_loss: 0.0202 - val_mae: 0.0569 - val_mse: 0.0175 - learning_rate: 6.2500e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0337 - mae: 0.1416 - mse: 0.0385 - val_loss: 0.0230 - val_mae: 0.0756 - val_mse: 0.0202 - learning_rate: 6.2500e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 0.0318 - mae: 0.1310 - mse: 0.0356 - val_loss: 0.0216 - val_mae: 0.0864 - val_mse: 0.0189 - learning_rate: 6.2500e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0311 - mae: 0.1315 - mse: 0.0344 - val_loss: 0.0201 - val_mae: 0.0561 - val_mse: 0.0151 - learning_rate: 6.2500e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0309 - mae: 0.1294 - mse: 0.0344 - val_loss: 0.0269 - val_mae: 0.1261 - val_mse: 0.0321 - learning_rate: 6.2500e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.0307 - mae: 0.1300 - mse: 0.0368 - val_loss: 0.0219 - val_mae: 0.0789 - val_mse: 0.0203 - learning_rate: 6.2500e-05\n",
      "Epoch 97/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.0305 - mae: 0.1274 - mse: 0.0326 - val_loss: 0.0243 - val_mae: 0.1108 - val_mse: 0.0252 - learning_rate: 6.2500e-05\n",
      "Epoch 98/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0298 - mae: 0.1276 - mse: 0.0328 - val_loss: 0.0225 - val_mae: 0.0791 - val_mse: 0.0235 - learning_rate: 6.2500e-05\n",
      "Epoch 99/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.0309 - mae: 0.1323 - mse: 0.0346 - val_loss: 0.0261 - val_mae: 0.1232 - val_mse: 0.0283 - learning_rate: 6.2500e-05\n",
      "Epoch 100/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0297 - mae: 0.1274 - mse: 0.0315 - val_loss: 0.0213 - val_mae: 0.0743 - val_mse: 0.0191 - learning_rate: 6.2500e-05\n",
      "Epoch 101/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0309 - mae: 0.1325 - mse: 0.0361 - val_loss: 0.0215 - val_mae: 0.0810 - val_mse: 0.0204 - learning_rate: 6.2500e-05\n",
      "Epoch 102/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.0326 - mae: 0.1330 - mse: 0.0394 - val_loss: 0.0223 - val_mae: 0.0934 - val_mse: 0.0230 - learning_rate: 6.2500e-05\n",
      "Epoch 103/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.0281 - mae: 0.1196 - mse: 0.0323 - val_loss: 0.0252 - val_mae: 0.1163 - val_mse: 0.0266 - learning_rate: 6.2500e-05\n",
      "Epoch 104/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0353 - mae: 0.1404 - mse: 0.0469\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.0352 - mae: 0.1404 - mse: 0.0469 - val_loss: 0.0222 - val_mae: 0.0886 - val_mse: 0.0212 - learning_rate: 6.2500e-05\n",
      "Epoch 105/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0302 - mae: 0.1285 - mse: 0.0347 - val_loss: 0.0231 - val_mae: 0.1033 - val_mse: 0.0238 - learning_rate: 3.1250e-05\n",
      "Epoch 106/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0291 - mae: 0.1246 - mse: 0.0327 - val_loss: 0.0200 - val_mae: 0.0801 - val_mse: 0.0205 - learning_rate: 3.1250e-05\n",
      "Epoch 107/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 97ms/step - loss: 0.0284 - mae: 0.1262 - mse: 0.0318 - val_loss: 0.0203 - val_mae: 0.0797 - val_mse: 0.0200 - learning_rate: 3.1250e-05\n",
      "Epoch 108/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0285 - mae: 0.1236 - mse: 0.0352 - val_loss: 0.0203 - val_mae: 0.0773 - val_mse: 0.0200 - learning_rate: 3.1250e-05\n",
      "Epoch 109/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0313 - mae: 0.1293 - mse: 0.0470 - val_loss: 0.0221 - val_mae: 0.0928 - val_mse: 0.0224 - learning_rate: 3.1250e-05\n",
      "Epoch 110/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 0.0278 - mae: 0.1217 - mse: 0.0307 - val_loss: 0.0224 - val_mae: 0.1005 - val_mse: 0.0233 - learning_rate: 3.1250e-05\n",
      "Epoch 111/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 0.0262 - mae: 0.1184 - mse: 0.0271 - val_loss: 0.0189 - val_mae: 0.0578 - val_mse: 0.0177 - learning_rate: 3.1250e-05\n",
      "Epoch 112/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 0.0267 - mae: 0.1199 - mse: 0.0289 - val_loss: 0.0197 - val_mae: 0.0779 - val_mse: 0.0195 - learning_rate: 3.1250e-05\n",
      "Epoch 113/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0262 - mae: 0.1163 - mse: 0.0283 - val_loss: 0.0209 - val_mae: 0.0841 - val_mse: 0.0219 - learning_rate: 3.1250e-05\n",
      "Epoch 114/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0260 - mae: 0.1171 - mse: 0.0282 - val_loss: 0.0224 - val_mae: 0.1119 - val_mse: 0.0247 - learning_rate: 3.1250e-05\n",
      "Epoch 115/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 0.0271 - mae: 0.1209 - mse: 0.0301 - val_loss: 0.0188 - val_mae: 0.0652 - val_mse: 0.0186 - learning_rate: 3.1250e-05\n",
      "Epoch 116/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0292 - mae: 0.1248 - mse: 0.0382 - val_loss: 0.0221 - val_mae: 0.0993 - val_mse: 0.0238 - learning_rate: 3.1250e-05\n",
      "Epoch 117/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0273 - mae: 0.1227 - mse: 0.0304 - val_loss: 0.0220 - val_mae: 0.0980 - val_mse: 0.0246 - learning_rate: 3.1250e-05\n",
      "Epoch 118/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0272 - mae: 0.1239 - mse: 0.0299 - val_loss: 0.0203 - val_mae: 0.0870 - val_mse: 0.0205 - learning_rate: 3.1250e-05\n",
      "Epoch 119/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0280 - mae: 0.1239 - mse: 0.0353 - val_loss: 0.0201 - val_mae: 0.0833 - val_mse: 0.0202 - learning_rate: 3.1250e-05\n",
      "Epoch 120/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.0268 - mae: 0.1212 - mse: 0.0303 - val_loss: 0.0188 - val_mae: 0.0669 - val_mse: 0.0188 - learning_rate: 3.1250e-05\n",
      "Epoch 121/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0272 - mae: 0.1241 - mse: 0.0307 - val_loss: 0.0193 - val_mae: 0.0738 - val_mse: 0.0195 - learning_rate: 3.1250e-05\n",
      "Epoch 122/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 85ms/step - loss: 0.0241 - mae: 0.1093 - mse: 0.0263 - val_loss: 0.0198 - val_mae: 0.0795 - val_mse: 0.0210 - learning_rate: 3.1250e-05\n",
      "Epoch 123/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0281 - mae: 0.1239 - mse: 0.0327 - val_loss: 0.0192 - val_mae: 0.0684 - val_mse: 0.0196 - learning_rate: 3.1250e-05\n",
      "Epoch 124/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0291 - mae: 0.1223 - mse: 0.0435 - val_loss: 0.0199 - val_mae: 0.0792 - val_mse: 0.0203 - learning_rate: 3.1250e-05\n",
      "Epoch 125/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0256 - mae: 0.1167 - mse: 0.0278\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 78ms/step - loss: 0.0256 - mae: 0.1167 - mse: 0.0278 - val_loss: 0.0197 - val_mae: 0.0846 - val_mse: 0.0200 - learning_rate: 3.1250e-05\n",
      "Epoch 126/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 0.0256 - mae: 0.1180 - mse: 0.0285 - val_loss: 0.0187 - val_mae: 0.0761 - val_mse: 0.0190 - learning_rate: 1.5625e-05\n",
      "Epoch 127/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 92ms/step - loss: 0.0256 - mae: 0.1187 - mse: 0.0280 - val_loss: 0.0190 - val_mae: 0.0796 - val_mse: 0.0195 - learning_rate: 1.5625e-05\n",
      "Epoch 128/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 94ms/step - loss: 0.0281 - mae: 0.1218 - mse: 0.0373 - val_loss: 0.0191 - val_mae: 0.0843 - val_mse: 0.0200 - learning_rate: 1.5625e-05\n",
      "Epoch 129/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0281 - mae: 0.1197 - mse: 0.0408 - val_loss: 0.0195 - val_mae: 0.0871 - val_mse: 0.0200 - learning_rate: 1.5625e-05\n",
      "Epoch 130/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0274 - mae: 0.1215 - mse: 0.0325 - val_loss: 0.0189 - val_mae: 0.0784 - val_mse: 0.0195 - learning_rate: 1.5625e-05\n",
      "Epoch 131/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 0.0254 - mae: 0.1165 - mse: 0.0288 - val_loss: 0.0193 - val_mae: 0.0837 - val_mse: 0.0199 - learning_rate: 1.5625e-05\n",
      "Epoch 132/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0245 - mae: 0.1137 - mse: 0.0270 - val_loss: 0.0203 - val_mae: 0.0950 - val_mse: 0.0222 - learning_rate: 1.5625e-05\n",
      "Epoch 133/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0231 - mae: 0.1126 - mse: 0.0238 - val_loss: 0.0203 - val_mae: 0.0914 - val_mse: 0.0226 - learning_rate: 1.5625e-05\n",
      "Epoch 134/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 79ms/step - loss: 0.0258 - mae: 0.1193 - mse: 0.0298 - val_loss: 0.0188 - val_mae: 0.0792 - val_mse: 0.0194 - learning_rate: 1.5625e-05\n",
      "Epoch 135/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.0264 - mae: 0.1182 - mse: 0.0327\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 79ms/step - loss: 0.0264 - mae: 0.1181 - mse: 0.0327 - val_loss: 0.0188 - val_mae: 0.0778 - val_mse: 0.0199 - learning_rate: 1.5625e-05\n",
      "Epoch 136/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0258 - mae: 0.1185 - mse: 0.0327 - val_loss: 0.0181 - val_mae: 0.0735 - val_mse: 0.0184 - learning_rate: 7.8125e-06\n",
      "Epoch 137/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 78ms/step - loss: 0.0231 - mae: 0.1134 - mse: 0.0240 - val_loss: 0.0186 - val_mae: 0.0816 - val_mse: 0.0199 - learning_rate: 7.8125e-06\n",
      "Epoch 138/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 79ms/step - loss: 0.0256 - mae: 0.1197 - mse: 0.0327 - val_loss: 0.0180 - val_mae: 0.0727 - val_mse: 0.0188 - learning_rate: 7.8125e-06\n",
      "Epoch 139/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0257 - mae: 0.1144 - mse: 0.0376 - val_loss: 0.0186 - val_mae: 0.0810 - val_mse: 0.0198 - learning_rate: 7.8125e-06\n",
      "Epoch 140/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0241 - mae: 0.1139 - mse: 0.0270 - val_loss: 0.0192 - val_mae: 0.0866 - val_mse: 0.0207 - learning_rate: 7.8125e-06\n",
      "Epoch 141/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 78ms/step - loss: 0.0252 - mae: 0.1169 - mse: 0.0282 - val_loss: 0.0190 - val_mae: 0.0817 - val_mse: 0.0202 - learning_rate: 7.8125e-06\n",
      "Epoch 142/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 78ms/step - loss: 0.0237 - mae: 0.1141 - mse: 0.0251 - val_loss: 0.0180 - val_mae: 0.0711 - val_mse: 0.0183 - learning_rate: 7.8125e-06\n",
      "Epoch 143/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0254 - mae: 0.1205 - mse: 0.0287 - val_loss: 0.0183 - val_mae: 0.0738 - val_mse: 0.0195 - learning_rate: 7.8125e-06\n",
      "Epoch 144/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 78ms/step - loss: 0.0242 - mae: 0.1164 - mse: 0.0270 - val_loss: 0.0182 - val_mae: 0.0760 - val_mse: 0.0191 - learning_rate: 7.8125e-06\n",
      "Epoch 145/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0255 - mae: 0.1198 - mse: 0.0296 - val_loss: 0.0190 - val_mae: 0.0859 - val_mse: 0.0206 - learning_rate: 7.8125e-06\n",
      "Epoch 146/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 81ms/step - loss: 0.0253 - mae: 0.1158 - mse: 0.0315 - val_loss: 0.0189 - val_mae: 0.0824 - val_mse: 0.0203 - learning_rate: 7.8125e-06\n",
      "Epoch 147/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0264 - mae: 0.1179 - mse: 0.0360 - val_loss: 0.0190 - val_mae: 0.0866 - val_mse: 0.0206 - learning_rate: 7.8125e-06\n",
      "Epoch 148/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0266 - mae: 0.1231 - mse: 0.0335\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0266 - mae: 0.1231 - mse: 0.0335 - val_loss: 0.0204 - val_mae: 0.0975 - val_mse: 0.0233 - learning_rate: 7.8125e-06\n",
      "Epoch 149/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 80ms/step - loss: 0.0241 - mae: 0.1139 - mse: 0.0266 - val_loss: 0.0180 - val_mae: 0.0771 - val_mse: 0.0190 - learning_rate: 3.9063e-06\n",
      "Epoch 150/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 78ms/step - loss: 0.0241 - mae: 0.1140 - mse: 0.0267 - val_loss: 0.0184 - val_mae: 0.0795 - val_mse: 0.0196 - learning_rate: 3.9063e-06\n",
      "Epoch 151/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 79ms/step - loss: 0.0237 - mae: 0.1146 - mse: 0.0257 - val_loss: 0.0177 - val_mae: 0.0732 - val_mse: 0.0185 - learning_rate: 3.9063e-06\n",
      "Epoch 152/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 79ms/step - loss: 0.0231 - mae: 0.1106 - mse: 0.0250 - val_loss: 0.0187 - val_mae: 0.0825 - val_mse: 0.0205 - learning_rate: 3.9063e-06\n",
      "Epoch 153/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 78ms/step - loss: 0.0248 - mae: 0.1160 - mse: 0.0285 - val_loss: 0.0185 - val_mae: 0.0796 - val_mse: 0.0199 - learning_rate: 3.9063e-06\n",
      "Epoch 154/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0262 - mae: 0.1186 - mse: 0.0352 - val_loss: 0.0182 - val_mae: 0.0767 - val_mse: 0.0193 - learning_rate: 3.9063e-06\n",
      "Epoch 155/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - loss: 0.0263 - mae: 0.1175 - mse: 0.0335 - val_loss: 0.0190 - val_mae: 0.0885 - val_mse: 0.0211 - learning_rate: 3.9063e-06\n",
      "Epoch 156/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 76ms/step - loss: 0.0238 - mae: 0.1139 - mse: 0.0270 - val_loss: 0.0191 - val_mae: 0.0869 - val_mse: 0.0209 - learning_rate: 3.9063e-06\n",
      "Epoch 157/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0237 - mae: 0.1128 - mse: 0.0265 - val_loss: 0.0189 - val_mae: 0.0858 - val_mse: 0.0205 - learning_rate: 3.9063e-06\n",
      "Epoch 158/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 103ms/step - loss: 0.0255 - mae: 0.1203 - mse: 0.0293 - val_loss: 0.0184 - val_mae: 0.0814 - val_mse: 0.0197 - learning_rate: 3.9063e-06\n",
      "Epoch 159/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 97ms/step - loss: 0.0236 - mae: 0.1095 - mse: 0.0296 - val_loss: 0.0186 - val_mae: 0.0849 - val_mse: 0.0203 - learning_rate: 3.9063e-06\n",
      "Epoch 160/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 79ms/step - loss: 0.0224 - mae: 0.1102 - mse: 0.0233 - val_loss: 0.0182 - val_mae: 0.0807 - val_mse: 0.0196 - learning_rate: 3.9063e-06\n",
      "Epoch 161/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0244 - mae: 0.1120 - mse: 0.0313\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 84ms/step - loss: 0.0244 - mae: 0.1120 - mse: 0.0313 - val_loss: 0.0185 - val_mae: 0.0821 - val_mse: 0.0198 - learning_rate: 3.9063e-06\n",
      "Epoch 162/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.0232 - mae: 0.1109 - mse: 0.0272 - val_loss: 0.0184 - val_mae: 0.0822 - val_mse: 0.0199 - learning_rate: 1.9531e-06\n",
      "Epoch 163/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 0.0246 - mae: 0.1139 - mse: 0.0308 - val_loss: 0.0184 - val_mae: 0.0829 - val_mse: 0.0199 - learning_rate: 1.9531e-06\n",
      "Epoch 164/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - loss: 0.0238 - mae: 0.1117 - mse: 0.0267 - val_loss: 0.0182 - val_mae: 0.0805 - val_mse: 0.0194 - learning_rate: 1.9531e-06\n",
      "Epoch 165/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - loss: 0.0227 - mae: 0.1075 - mse: 0.0247 - val_loss: 0.0192 - val_mae: 0.0909 - val_mse: 0.0213 - learning_rate: 1.9531e-06\n",
      "Epoch 166/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - loss: 0.0244 - mae: 0.1145 - mse: 0.0271 - val_loss: 0.0183 - val_mae: 0.0803 - val_mse: 0.0196 - learning_rate: 1.9531e-06\n",
      "Epoch 167/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0233 - mae: 0.1137 - mse: 0.0250 - val_loss: 0.0180 - val_mae: 0.0791 - val_mse: 0.0192 - learning_rate: 1.9531e-06\n",
      "Epoch 168/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - loss: 0.0234 - mae: 0.1142 - mse: 0.0254 - val_loss: 0.0184 - val_mae: 0.0831 - val_mse: 0.0201 - learning_rate: 1.9531e-06\n",
      "Epoch 169/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0262 - mae: 0.1200 - mse: 0.0320 - val_loss: 0.0188 - val_mae: 0.0862 - val_mse: 0.0208 - learning_rate: 1.9531e-06\n",
      "Epoch 170/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0259 - mae: 0.1197 - mse: 0.0350 - val_loss: 0.0182 - val_mae: 0.0813 - val_mse: 0.0196 - learning_rate: 1.9531e-06\n",
      "Epoch 171/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0252 - mae: 0.1153 - mse: 0.0314\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - loss: 0.0252 - mae: 0.1153 - mse: 0.0314 - val_loss: 0.0180 - val_mae: 0.0786 - val_mse: 0.0192 - learning_rate: 1.9531e-06\n",
      "Epoch 172/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 81ms/step - loss: 0.0239 - mae: 0.1118 - mse: 0.0307 - val_loss: 0.0180 - val_mae: 0.0793 - val_mse: 0.0193 - learning_rate: 9.7656e-07\n",
      "Epoch 173/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 84ms/step - loss: 0.0244 - mae: 0.1139 - mse: 0.0294 - val_loss: 0.0178 - val_mae: 0.0765 - val_mse: 0.0189 - learning_rate: 9.7656e-07\n",
      "Epoch 174/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 84ms/step - loss: 0.0237 - mae: 0.1146 - mse: 0.0263 - val_loss: 0.0178 - val_mae: 0.0753 - val_mse: 0.0188 - learning_rate: 9.7656e-07\n",
      "Epoch 175/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0256 - mae: 0.1189 - mse: 0.0335 - val_loss: 0.0179 - val_mae: 0.0775 - val_mse: 0.0191 - learning_rate: 9.7656e-07\n",
      "Epoch 176/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.0273 - mae: 0.1198 - mse: 0.0404 - val_loss: 0.0177 - val_mae: 0.0748 - val_mse: 0.0188 - learning_rate: 9.7656e-07\n",
      "Epoch 177/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.0258 - mae: 0.1189 - mse: 0.0335 - val_loss: 0.0185 - val_mae: 0.0850 - val_mse: 0.0204 - learning_rate: 9.7656e-07\n",
      "Epoch 178/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.0274 - mae: 0.1225 - mse: 0.0387 - val_loss: 0.0186 - val_mae: 0.0873 - val_mse: 0.0204 - learning_rate: 9.7656e-07\n",
      "Epoch 179/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - loss: 0.0234 - mae: 0.1092 - mse: 0.0274 - val_loss: 0.0184 - val_mae: 0.0830 - val_mse: 0.0199 - learning_rate: 9.7656e-07\n",
      "Epoch 180/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - loss: 0.0235 - mae: 0.1144 - mse: 0.0266 - val_loss: 0.0179 - val_mae: 0.0776 - val_mse: 0.0190 - learning_rate: 9.7656e-07\n",
      "Epoch 181/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0244 - mae: 0.1150 - mse: 0.0288\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 93ms/step - loss: 0.0244 - mae: 0.1150 - mse: 0.0288 - val_loss: 0.0183 - val_mae: 0.0826 - val_mse: 0.0198 - learning_rate: 9.7656e-07\n",
      "Epoch 182/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 80ms/step - loss: 0.0242 - mae: 0.1128 - mse: 0.0290 - val_loss: 0.0183 - val_mae: 0.0822 - val_mse: 0.0198 - learning_rate: 4.8828e-07\n",
      "Epoch 183/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - loss: 0.0238 - mae: 0.1140 - mse: 0.0281 - val_loss: 0.0181 - val_mae: 0.0796 - val_mse: 0.0195 - learning_rate: 4.8828e-07\n",
      "Epoch 184/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - loss: 0.0233 - mae: 0.1098 - mse: 0.0275 - val_loss: 0.0181 - val_mae: 0.0807 - val_mse: 0.0197 - learning_rate: 4.8828e-07\n",
      "Epoch 185/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0231 - mae: 0.1090 - mse: 0.0277 - val_loss: 0.0180 - val_mae: 0.0793 - val_mse: 0.0194 - learning_rate: 4.8828e-07\n",
      "Epoch 186/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0251 - mae: 0.1207 - mse: 0.0305 - val_loss: 0.0178 - val_mae: 0.0764 - val_mse: 0.0190 - learning_rate: 4.8828e-07\n",
      "Epoch 187/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0241 - mae: 0.1152 - mse: 0.0279 - val_loss: 0.0182 - val_mae: 0.0807 - val_mse: 0.0197 - learning_rate: 4.8828e-07\n",
      "Epoch 188/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0239 - mae: 0.1146 - mse: 0.0271 - val_loss: 0.0182 - val_mae: 0.0811 - val_mse: 0.0198 - learning_rate: 4.8828e-07\n",
      "Epoch 189/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 84ms/step - loss: 0.0235 - mae: 0.1125 - mse: 0.0278 - val_loss: 0.0182 - val_mae: 0.0807 - val_mse: 0.0199 - learning_rate: 4.8828e-07\n",
      "Epoch 190/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0249 - mae: 0.1192 - mse: 0.0306 - val_loss: 0.0185 - val_mae: 0.0838 - val_mse: 0.0202 - learning_rate: 4.8828e-07\n",
      "Epoch 191/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0264 - mae: 0.1190 - mse: 0.0355\n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 93ms/step - loss: 0.0264 - mae: 0.1190 - mse: 0.0355 - val_loss: 0.0183 - val_mae: 0.0828 - val_mse: 0.0199 - learning_rate: 4.8828e-07\n",
      "Epoch 192/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 88ms/step - loss: 0.0220 - mae: 0.1076 - mse: 0.0230 - val_loss: 0.0180 - val_mae: 0.0788 - val_mse: 0.0193 - learning_rate: 2.4414e-07\n",
      "Epoch 193/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 93ms/step - loss: 0.0228 - mae: 0.1104 - mse: 0.0244 - val_loss: 0.0183 - val_mae: 0.0825 - val_mse: 0.0200 - learning_rate: 2.4414e-07\n",
      "Epoch 194/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 90ms/step - loss: 0.0228 - mae: 0.1086 - mse: 0.0246 - val_loss: 0.0179 - val_mae: 0.0778 - val_mse: 0.0193 - learning_rate: 2.4414e-07\n",
      "Epoch 195/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - loss: 0.0233 - mae: 0.1122 - mse: 0.0260 - val_loss: 0.0183 - val_mae: 0.0826 - val_mse: 0.0199 - learning_rate: 2.4414e-07\n",
      "Epoch 196/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 86ms/step - loss: 0.0241 - mae: 0.1156 - mse: 0.0280 - val_loss: 0.0184 - val_mae: 0.0833 - val_mse: 0.0201 - learning_rate: 2.4414e-07\n",
      "Epoch 197/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0242 - mae: 0.1152 - mse: 0.0284 - val_loss: 0.0183 - val_mae: 0.0838 - val_mse: 0.0201 - learning_rate: 2.4414e-07\n",
      "Epoch 198/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 82ms/step - loss: 0.0249 - mae: 0.1142 - mse: 0.0333 - val_loss: 0.0183 - val_mae: 0.0821 - val_mse: 0.0199 - learning_rate: 2.4414e-07\n",
      "Epoch 199/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 79ms/step - loss: 0.0262 - mae: 0.1158 - mse: 0.0365 - val_loss: 0.0184 - val_mae: 0.0846 - val_mse: 0.0202 - learning_rate: 2.4414e-07\n",
      "Epoch 200/200\n",
      "\u001b[1m277/277\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 79ms/step - loss: 0.0271 - mae: 0.1224 - mse: 0.0346 - val_loss: 0.0182 - val_mae: 0.0810 - val_mse: 0.0197 - learning_rate: 2.4414e-07\n",
      "Enhanced model training completed!\n",
      "Evaluating enhanced model...\n",
      "Enhanced Model Evaluation Metrics:\n",
      "Training RMSE: $11.9122\n",
      "Testing RMSE: $9.8376\n",
      "Training MAE: $5.4208\n",
      "Testing MAE: $8.4399\n",
      "Training R²: 0.9944\n",
      "Testing R²: 0.8970\n",
      "Training MAPE: 4.62%\n",
      "Testing MAPE: 4.62%\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EnhancedStockPredictor' object has no attribute 'predict_future'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 357\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Enhanced Stock Predictor...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will take longer but should achieve better MAE!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 357\u001b[0m predictor, history, metrics, predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_enhanced_stock_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAPI_KEY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlstm_advanced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEnhanced training completed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting MAE: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_mae\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 336\u001b[0m, in \u001b[0;36mtrain_enhanced_stock_model\u001b[1;34m(ticker_symbol, api_key, model_type, save_model)\u001b[0m\n\u001b[0;32m    333\u001b[0m metrics \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mevaluate_model()\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m# Make future predictions\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m future_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_future\u001b[49m(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFuture Predictions (Next 10 days):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(future_predictions, \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EnhancedStockPredictor' object has no attribute 'predict_future'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, GRU, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedStockPredictor:\n",
    "    \n",
    "    def __init__(self, ticker, api_key, sequence_length=120, test_size=0.15):\n",
    "        self.ticker = ticker\n",
    "        self.api_key = api_key\n",
    "        self.data = None\n",
    "        self.sequence_length = sequence_length  # Increased for more context\n",
    "        self.test_size = test_size  # Reduced for more training data\n",
    "        self.scaler = RobustScaler()  # More robust to outliers\n",
    "        self.model = None\n",
    "        self.scaled_data = None\n",
    "        self.features = None\n",
    "\n",
    "    def fetch_data(self, period=\"5y\"):\n",
    "        \"\"\"Enhanced data fetching with more features\"\"\"\n",
    "        print(f\"Attempting to fetch data for {self.ticker} using yfinance...\")\n",
    "\n",
    "        try:\n",
    "            self.data = yf.download(self.ticker, period=period, threads=False)\n",
    "            if self.data.empty:\n",
    "                raise ValueError(\"yfinance returned empty data.\")\n",
    "            print(\"Data fetched from yfinance.\")\n",
    "        except Exception as e:\n",
    "            print(f\"yfinance failed: {e}\")\n",
    "            print(f\"Falling back to Alpha Vantage for {self.ticker}...\")\n",
    "            self.data = self._fetch_from_alpha_vantage(period)\n",
    "            print(\"Data fetched from Alpha Vantage.\")\n",
    "\n",
    "        # Enhanced technical indicators\n",
    "        self.data['MA_5'] = self.data['Close'].rolling(window=5).mean()\n",
    "        self.data['MA_10'] = self.data['Close'].rolling(window=10).mean()\n",
    "        self.data['MA_20'] = self.data['Close'].rolling(window=20).mean()\n",
    "        self.data['MA_50'] = self.data['Close'].rolling(window=50).mean()\n",
    "        self.data['MA_200'] = self.data['Close'].rolling(window=200).mean()\n",
    "        \n",
    "        # More technical indicators\n",
    "        self.data['RSI'] = self._calculate_rsi(self.data['Close'])\n",
    "        self.data['MACD'], self.data['MACD_signal'] = self._calculate_macd(self.data['Close'])\n",
    "        self.data['BB_upper'], self.data['BB_lower'] = self._calculate_bollinger_bands(self.data['Close'])\n",
    "        self.data['ATR'] = self._calculate_atr(self.data['High'], self.data['Low'], self.data['Close'])\n",
    "        \n",
    "        # Price-based features\n",
    "        self.data['Price_Change'] = self.data['Close'].pct_change()\n",
    "        self.data['High_Low_Ratio'] = self.data['High'] / self.data['Low']\n",
    "        self.data['Volume_MA'] = self.data['Volume'].rolling(window=20).mean()\n",
    "        self.data['Volume_Ratio'] = self.data['Volume'] / self.data['Volume_MA']\n",
    "        \n",
    "        # Lag features\n",
    "        for lag in [1, 2, 3, 5]:\n",
    "            self.data[f'Close_lag_{lag}'] = self.data['Close'].shift(lag)\n",
    "            self.data[f'Volume_lag_{lag}'] = self.data['Volume'].shift(lag)\n",
    "        \n",
    "        # Clean up\n",
    "        self.data = self.data.dropna()\n",
    "        print(f\"Final data shape: {self.data.shape}\")\n",
    "\n",
    "    def _fetch_from_alpha_vantage(self, period):\n",
    "        ts = TimeSeries(key=self.api_key, output_format='pandas')\n",
    "        outputsize = 'full' if period in ['2y', '5y', 'max'] else 'compact'\n",
    "        \n",
    "        try:\n",
    "            data, _ = ts.get_daily(symbol=self.ticker, outputsize=outputsize)\n",
    "            data.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "            data = data.sort_index()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Alpha Vantage failed: {e}\")\n",
    "        \n",
    "        if data.empty:\n",
    "            raise ValueError(f\"No data found for {self.ticker} from Alpha Vantage.\")\n",
    "        return data\n",
    "\n",
    "    def _calculate_rsi(self, series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "        loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    def _calculate_macd(self, series, fast=12, slow=26, signal=9):\n",
    "        ema_fast = series.ewm(span=fast).mean()\n",
    "        ema_slow = series.ewm(span=slow).mean()\n",
    "        macd = ema_fast - ema_slow\n",
    "        macd_signal = macd.ewm(span=signal).mean()\n",
    "        return macd, macd_signal\n",
    "    \n",
    "    def _calculate_bollinger_bands(self, series, window=20, std_dev=2):\n",
    "        rolling_mean = series.rolling(window=window).mean()\n",
    "        rolling_std = series.rolling(window=window).std()\n",
    "        upper_band = rolling_mean + (rolling_std * std_dev)\n",
    "        lower_band = rolling_mean - (rolling_std * std_dev)\n",
    "        return upper_band, lower_band\n",
    "    \n",
    "    def _calculate_atr(self, high, low, close, period=14):\n",
    "        high_low = high - low\n",
    "        high_close = np.abs(high - close.shift())\n",
    "        low_close = np.abs(low - close.shift())\n",
    "        ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "        true_range = np.max(ranges, axis=1)\n",
    "        return true_range.rolling(period).mean()\n",
    "    \n",
    "    def prepare_data(self, features=None):\n",
    "        \"\"\"Enhanced data preparation with more features\"\"\"\n",
    "        if features is None:\n",
    "            features = [\n",
    "                'Close', 'Open', 'High', 'Low', 'Volume',\n",
    "                'MA_5', 'MA_10', 'MA_20', 'MA_50', 'MA_200',\n",
    "                'RSI', 'MACD', 'MACD_signal', 'BB_upper', 'BB_lower', 'ATR',\n",
    "                'Price_Change', 'High_Low_Ratio', 'Volume_Ratio',\n",
    "                'Close_lag_1', 'Close_lag_2', 'Close_lag_3', 'Close_lag_5',\n",
    "                'Volume_lag_1', 'Volume_lag_2', 'Volume_lag_3', 'Volume_lag_5'\n",
    "            ]\n",
    "        \n",
    "        print(\"Preparing enhanced data...\")\n",
    "        \n",
    "        # Select available features (some might not exist if data is limited)\n",
    "        available_features = [f for f in features if f in self.data.columns]\n",
    "        print(f\"Using {len(available_features)} features: {available_features}\")\n",
    "        \n",
    "        # Store features for later use\n",
    "        self.features = available_features\n",
    "        \n",
    "        # Select features\n",
    "        feature_data = self.data[available_features].values\n",
    "        \n",
    "        # Scale the data\n",
    "        self.scaled_data = self.scaler.fit_transform(feature_data)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(self.sequence_length, len(self.scaled_data)):\n",
    "            X.append(self.scaled_data[i-self.sequence_length:i])\n",
    "            y.append(self.scaled_data[i, 0])  # Predict 'Close' price (first feature)\n",
    "            \n",
    "        X, y = np.array(X), np.array(y)\n",
    "        \n",
    "        # Split data\n",
    "        split_idx = int(len(X) * (1 - self.test_size))\n",
    "        self.X_train = X[:split_idx]\n",
    "        self.X_test = X[split_idx:]\n",
    "        self.y_train = y[:split_idx]\n",
    "        self.y_test = y[split_idx:]\n",
    "        \n",
    "        print(f\"Training data shape: X={self.X_train.shape}, y={self.y_train.shape}\")\n",
    "        print(f\"Testing data shape: X={self.X_test.shape}, y={self.y_test.shape}\")\n",
    "        \n",
    "    def build_enhanced_model(self, model_type='lstm_advanced'):\n",
    "        \"\"\"Build enhanced model architectures\"\"\"\n",
    "        print(f\"Building enhanced model: {model_type}\")\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        \n",
    "        if model_type == 'lstm_advanced':\n",
    "            # Advanced LSTM with Bidirectional layers\n",
    "            self.model.add(Bidirectional(LSTM(128, return_sequences=True), \n",
    "                                       input_shape=(self.X_train.shape[1], self.X_train.shape[2])))\n",
    "            self.model.add(BatchNormalization())\n",
    "            self.model.add(Dropout(0.2))\n",
    "            \n",
    "            self.model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "            self.model.add(BatchNormalization())\n",
    "            self.model.add(Dropout(0.2))\n",
    "            \n",
    "            self.model.add(LSTM(32, return_sequences=False))\n",
    "            self.model.add(BatchNormalization())\n",
    "            self.model.add(Dropout(0.2))\n",
    "            \n",
    "        elif model_type == 'gru_ensemble':\n",
    "            # GRU-based architecture\n",
    "            self.model.add(GRU(128, return_sequences=True, \n",
    "                              input_shape=(self.X_train.shape[1], self.X_train.shape[2])))\n",
    "            self.model.add(BatchNormalization())\n",
    "            self.model.add(Dropout(0.3))\n",
    "            \n",
    "            self.model.add(GRU(64, return_sequences=True))\n",
    "            self.model.add(BatchNormalization())\n",
    "            self.model.add(Dropout(0.3))\n",
    "            \n",
    "            self.model.add(GRU(32))\n",
    "            self.model.add(BatchNormalization())\n",
    "            self.model.add(Dropout(0.3))\n",
    "        \n",
    "        # Dense layers with regularization\n",
    "        self.model.add(Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.3))\n",
    "        \n",
    "        self.model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.2))\n",
    "        \n",
    "        self.model.add(Dense(16, activation='relu'))\n",
    "        self.model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "        # Compile with different optimizers and loss functions\n",
    "        self.model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999),\n",
    "            loss='huber',  # More robust to outliers than MSE\n",
    "            metrics=['mae', 'mse']\n",
    "        )\n",
    "        \n",
    "        print(\"Enhanced model built successfully!\")\n",
    "        print(self.model.summary())\n",
    "        \n",
    "    def train_enhanced_model(self, epochs=200, batch_size=16, validation_split=0.15):\n",
    "        \"\"\"Enhanced training with better callbacks\"\"\"\n",
    "        print(\"Training enhanced model...\")\n",
    "        \n",
    "        # Enhanced callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=25,  # Increased patience\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-6\n",
    "        )\n",
    "        \n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            f'{self.ticker}_best_enhanced_model.keras', \n",
    "            monitor='val_loss', \n",
    "            save_best_only=True,\n",
    "            save_weights_only=False\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "            verbose=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(\"Enhanced model training completed!\")\n",
    "        return history\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Enhanced evaluation with more metrics\"\"\"\n",
    "        print(\"Evaluating enhanced model...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        train_predictions = self.model.predict(self.X_train, verbose=0)\n",
    "        test_predictions = self.model.predict(self.X_test, verbose=0)\n",
    "        \n",
    "        # Inverse transform predictions (more robust method)\n",
    "        def inverse_transform_predictions(predictions, original_shape):\n",
    "            # Create dummy array with same shape as original features\n",
    "            dummy_features = np.zeros((len(predictions), original_shape))\n",
    "            dummy_features[:, 0] = predictions.flatten()\n",
    "            # Inverse transform and return only the first column (Close price)\n",
    "            return self.scaler.inverse_transform(dummy_features)[:, 0]\n",
    "        \n",
    "        train_pred_original = inverse_transform_predictions(train_predictions, self.scaled_data.shape[1])\n",
    "        test_pred_original = inverse_transform_predictions(test_predictions, self.scaled_data.shape[1])\n",
    "        \n",
    "        # Inverse transform actual values\n",
    "        train_actual_original = inverse_transform_predictions(self.y_train.reshape(-1, 1), self.scaled_data.shape[1])\n",
    "        test_actual_original = inverse_transform_predictions(self.y_test.reshape(-1, 1), self.scaled_data.shape[1])\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        train_rmse = np.sqrt(mean_squared_error(train_actual_original, train_pred_original))\n",
    "        test_rmse = np.sqrt(mean_squared_error(test_actual_original, test_pred_original))\n",
    "        train_mae = mean_absolute_error(train_actual_original, train_pred_original)\n",
    "        test_mae = mean_absolute_error(test_actual_original, test_pred_original)\n",
    "        train_r2 = r2_score(train_actual_original, train_pred_original)\n",
    "        test_r2 = r2_score(test_actual_original, test_pred_original)\n",
    "        \n",
    "        # Additional metrics\n",
    "        train_mape = np.mean(np.abs((train_actual_original - train_pred_original) / train_actual_original)) * 100\n",
    "        test_mape = np.mean(np.abs((test_actual_original - test_pred_original) / test_actual_original)) * 100\n",
    "        \n",
    "        metrics = {\n",
    "            'train_rmse': train_rmse,\n",
    "            'test_rmse': test_rmse,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'train_mape': train_mape,\n",
    "            'test_mape': test_mape\n",
    "        }\n",
    "        \n",
    "        print(\"Enhanced Model Evaluation Metrics:\")\n",
    "        print(f\"Training RMSE: ${train_rmse:.4f}\")\n",
    "        print(f\"Testing RMSE: ${test_rmse:.4f}\")\n",
    "        print(f\"Training MAE: ${train_mae:.4f}\")\n",
    "        print(f\"Testing MAE: ${test_mae:.4f}\")\n",
    "        print(f\"Training R²: {train_r2:.4f}\")\n",
    "        print(f\"Testing R²: {test_r2:.4f}\")\n",
    "        print(f\"Training MAPE: {train_mape:.2f}%\")\n",
    "        print(f\"Testing MAPE: {test_mape:.2f}%\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def predict_future(self, days=10):\n",
    "        \"\"\"\n",
    "        Predict future stock prices for the specified number of days\n",
    "        \"\"\"\n",
    "        print(f\"Predicting future prices for {days} days...\")\n",
    "        \n",
    "        # Get the last sequence_length days of scaled data\n",
    "        last_sequence = self.scaled_data[-self.sequence_length:].copy()\n",
    "        future_predictions = []\n",
    "        \n",
    "        for day in range(days):\n",
    "            # Reshape for prediction\n",
    "            current_sequence = last_sequence.reshape(1, self.sequence_length, len(self.features))\n",
    "            \n",
    "            # Make prediction\n",
    "            next_pred = self.model.predict(current_sequence, verbose=0)[0, 0]\n",
    "            future_predictions.append(next_pred)\n",
    "            \n",
    "            # Update the sequence for next prediction\n",
    "            # Create new row with predicted price and estimated other features\n",
    "            new_row = last_sequence[-1].copy()  # Copy last row\n",
    "            new_row[0] = next_pred  # Update the Close price (first feature)\n",
    "            \n",
    "            # For simplicity, we'll use the last known values for other features\n",
    "            # In a more sophisticated approach, you might predict these as well\n",
    "            \n",
    "            # Shift the sequence and add the new prediction\n",
    "            last_sequence = np.roll(last_sequence, -1, axis=0)\n",
    "            last_sequence[-1] = new_row\n",
    "        \n",
    "        # Inverse transform predictions to get actual prices\n",
    "        future_predictions = np.array(future_predictions).reshape(-1, 1)\n",
    "        \n",
    "        # Create dummy array for inverse transform\n",
    "        dummy_features = np.zeros((len(future_predictions), len(self.features)))\n",
    "        dummy_features[:, 0] = future_predictions.flatten()\n",
    "        \n",
    "        # Inverse transform to get actual prices\n",
    "        future_prices = self.scaler.inverse_transform(dummy_features)[:, 0]\n",
    "        \n",
    "        return future_prices\n",
    "    \n",
    "    def save_model(self):\n",
    "        \"\"\"Save the trained model and scaler\"\"\"\n",
    "        model_filename = f'{self.ticker}_enhanced_model.keras'\n",
    "        scaler_filename = f'{self.ticker}_scaler.pkl'\n",
    "        \n",
    "        # Save model\n",
    "        self.model.save(model_filename)\n",
    "        print(f\"Model saved as {model_filename}\")\n",
    "        \n",
    "        # Save scaler\n",
    "        joblib.dump(self.scaler, scaler_filename)\n",
    "        print(f\"Scaler saved as {scaler_filename}\")\n",
    "        \n",
    "        # Save additional metadata\n",
    "        metadata = {\n",
    "            'features': self.features,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'ticker': self.ticker\n",
    "        }\n",
    "        \n",
    "        metadata_filename = f'{self.ticker}_metadata.pkl'\n",
    "        joblib.dump(metadata, metadata_filename)\n",
    "        print(f\"Metadata saved as {metadata_filename}\")\n",
    "    \n",
    "    def load_model(self, model_filename=None, scaler_filename=None, metadata_filename=None):\n",
    "        \"\"\"Load a previously trained model\"\"\"\n",
    "        if model_filename is None:\n",
    "            model_filename = f'{self.ticker}_enhanced_model.keras'\n",
    "        if scaler_filename is None:\n",
    "            scaler_filename = f'{self.ticker}_scaler.pkl'\n",
    "        if metadata_filename is None:\n",
    "            metadata_filename = f'{self.ticker}_metadata.pkl'\n",
    "        \n",
    "        # Load model\n",
    "        self.model = load_model(model_filename)\n",
    "        print(f\"Model loaded from {model_filename}\")\n",
    "        \n",
    "        # Load scaler\n",
    "        self.scaler = joblib.load(scaler_filename)\n",
    "        print(f\"Scaler loaded from {scaler_filename}\")\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata = joblib.load(metadata_filename)\n",
    "        self.features = metadata['features']\n",
    "        self.sequence_length = metadata['sequence_length']\n",
    "        print(f\"Metadata loaded from {metadata_filename}\")\n",
    "\n",
    "# Enhanced training function\n",
    "def train_enhanced_stock_model(ticker_symbol, api_key, model_type='lstm_advanced', save_model=True):\n",
    "    \"\"\"\n",
    "    Enhanced training pipeline for better MAE\n",
    "    \"\"\"\n",
    "    # Initialize enhanced predictor\n",
    "    predictor = EnhancedStockPredictor(ticker_symbol, api_key, sequence_length=120, test_size=0.15)\n",
    "    \n",
    "    # Fetch and prepare enhanced data\n",
    "    predictor.fetch_data(period=\"5y\")  # More data\n",
    "    predictor.prepare_data()\n",
    "    \n",
    "    # Build and train enhanced model\n",
    "    predictor.build_enhanced_model(model_type=model_type)\n",
    "    history = predictor.train_enhanced_model(epochs=200, batch_size=16)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = predictor.evaluate_model()\n",
    "    \n",
    "    # Make future predictions\n",
    "    future_predictions = predictor.predict_future(days=10)\n",
    "    \n",
    "    print(\"\\nFuture Predictions (Next 10 days):\")\n",
    "    for i, pred in enumerate(future_predictions, 1):\n",
    "        print(f\"Day {i}: ${pred:.2f}\")\n",
    "    \n",
    "    # Save model if requested\n",
    "    if save_model:\n",
    "        predictor.save_model()\n",
    "    \n",
    "    return predictor, history, metrics, future_predictions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your API key\n",
    "    API_KEY = \"JO6S4PKBXUZMZWE1\"\n",
    "    \n",
    "    # Example: Train enhanced model for Apple stock\n",
    "    ticker = \"AAPL\"\n",
    "    print(\"Training Enhanced Stock Predictor...\")\n",
    "    print(\"This will take longer but should achieve better MAE!\")\n",
    "    \n",
    "    predictor, history, metrics, predictions = train_enhanced_stock_model(\n",
    "        ticker, API_KEY, model_type='lstm_advanced', save_model=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEnhanced training completed for {ticker}\")\n",
    "    print(f\"Testing MAE: ${metrics['test_mae']:.4f}\")\n",
    "    print(\"Model files saved and ready for use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "Training Enhanced Stock Predictor...\n",
    "###\n",
    "This will take longer but should achieve better MAE!\n",
    "Attempting to fetch data for AAPL using yfinance...\n",
    "YF.download() has changed argument auto_adjust default to True\n",
    "[*********************100%***********************]  1 of 1 completed\n",
    "\n",
    "1 Failed download:\n",
    "['AAPL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
    "yfinance failed: yfinance returned empty data.\n",
    "Falling back to Alpha Vantage for AAPL...\n",
    "Data fetched from Alpha Vantage.\n",
    "Final data shape: (6242, 28)\n",
    "Preparing enhanced data...\n",
    "Using 27 features: ['Close', 'Open', 'High', 'Low', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'MA_50', 'MA_200', 'RSI', 'MACD', 'MACD_signal', 'BB_upper', 'BB_lower', 'ATR', 'Price_Change', 'High_Low_Ratio', 'Volume_Ratio', 'Close_lag_1', 'Close_lag_2', 'Close_lag_3', 'Close_lag_5', 'Volume_lag_1', 'Volume_lag_2', 'Volume_lag_3', 'Volume_lag_5']\n",
    "Training data shape: X=(5203, 120, 27), y=(5203,)\n",
    "Testing data shape: X=(919, 120, 27), y=(919,)\n",
    "Building enhanced model: lstm_advanced\n",
    "Enhanced model built successfully!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
